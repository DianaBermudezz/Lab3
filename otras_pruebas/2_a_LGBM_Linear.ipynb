{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "#from sklearn.metrics import cohen_kappa_score, accuracy_score,balanced_accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold # Use KFold for regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from plotly import express as px\n",
    "\n",
    "#from utils import plot_confusion_matrix\n",
    "\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "\n",
    "from joblib import load, dump\n",
    "import pyarrow.parquet as pq\n",
    "import joblib\n",
    "\n",
    "\n",
    "import gc\n",
    "import shutil\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armado Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Dataset/1_c_producto_train.parquet'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_DIR = '../'\n",
    "\n",
    "#PATH_TO_TRAIN = os.path.join(BASE_DIR, \"Dataset/train.parquet\")\n",
    "#PATH_TO_TRAIN = os.path.join(BASE_DIR, \"Dataset/train_producto.parquet\")\n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"Dataset/1_c_producto_train.parquet\")\n",
    "PATH_TO_MODELS = os.path.join(BASE_DIR, \"LGBM/models\")\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"LGBM/optuna_/optuna_temp_artifacts\")\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"LGBM/optuna_/optuna_artifacts\")\n",
    "PATH_TO_MODELS_OLD=PATH_TO_MODELS_OLD = os.path.join(BASE_DIR, \"LGBM/models/old\")\n",
    "\n",
    "\n",
    "nombrebase=\"sqlite:///db.sqlite33\"\n",
    "nombreestudio=\"producto_linear\"\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 50\n",
    "TEST_SIZE = 0.2\n",
    "PATH_TO_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(PATH_TO_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset['ultima_tn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float16')"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['clase_producto'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29652, 311)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat1', 'cat2', 'cat3', 'brand', 'categoria', 'estado_producto']\n"
     ]
    }
   ],
   "source": [
    "char_feats = dataset.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(char_feats)\n",
    "#numeric_feats = [f for f in dataset.columns if dataset[f].dtype!='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['periodo', 'product_id', 'producto_total_tn', 'avg_tn', 'std_tn', 'clientes_distintos', 'cust_request_qty', 'cust_request_tn', 'inicio_vida_p', 'fin_vida_p', 'sku_size', 'stock_final', 'dias_mes_4', 'dias_mes_3', 'anio', 'mes', 'trimestre', 'outlier', 'outlier-2', 'tn_lag_1', 'tn_lag_2', 'tn_lag_3', 'tn_lag_4', 'tn_lag_5', 'tn_lag_6', 'tn_lag_7', 'tn_lag_8', 'tn_lag_9', 'tn_lag_10', 'tn_lag_11', 'tn_lag_12', 'tn_lag_13', 'tn_lag_14', 'tn_lag_15', 'tn_lag_16', 'tn_lag_17', 'tn_lag_18', 'tn_lag_19', 'tn_lag_20', 'tn_lag_21', 'tn_lag_22', 'tn_lag_23', 'tn_lag_24', 'tn_lag_25', 'tn_lag_26', 'tn_lag_27', 'tn_lag_28', 'tn_lag_29', 'tn_lag_30', 'tn_lag_31', 'tn_lag_32', 'tn_lag_33', 'tn_lag_34', 'tn_lag_35', 'tn_lag_36', 'tn_media_movil_3', 'tn_media_movil_6', 'tn_media_movil_9', 'tn_media_movil_12', 'tn_std_movil_3', 'tn_std_movil_6', 'tn_std_movil_9', 'tn_std_movil_12', 'tn_min_movil_3', 'tn_min_movil_6', 'tn_min_movil_9', 'tn_min_movil_12', 'delta_media_movil_12', 'delta_media_movil_3', 'delta_media_movil_6', 'delta_media_movil_9', 'delta_std_movil_12', 'delta_std_movil_3', 'delta_std_movil_6', 'delta_std_movil_9', 'delta_min_movil_12', 'delta_min_movil_3', 'delta_min_movil_6', 'delta_min_movil_9', 'delta_tn_1', 'delta_tn_2', 'delta_tn_3', 'delta_tn_4', 'delta_tn_5', 'delta_tn_6', 'delta_tn_7', 'delta_tn_8', 'delta_tn_9', 'delta_tn_10', 'delta_tn_11', 'delta_tn_12', 'delta_tn_13', 'delta_tn_14', 'delta_tn_15', 'delta_tn_16', 'delta_tn_17', 'delta_tn_18', 'delta_tn_19', 'delta_tn_20', 'delta_tn_21', 'delta_tn_22', 'delta_tn_23', 'delta_tn_24', 'delta_tn_25', 'delta_tn_26', 'delta_tn_27', 'delta_tn_28', 'delta_tn_29', 'delta_tn_30', 'delta_tn_31', 'delta_tn_32', 'delta_tn_33', 'delta_tn_34', 'delta_tn_35', 'delta_tn_36', 'total_total_tn', 'total_avg_tn', 'total_std_tn', 'total_min_tn', 'total_max_tn', 'total_productos_distintos', 'total_clientes_distintos', 'total_total_tn_lag_1', 'total_total_tn_diff_1', 'total_total_tn_lag_2', 'total_total_tn_diff_2', 'total_total_tn_lag_3', 'total_total_tn_diff_3', 'total_total_tn_lag_4', 'total_total_tn_diff_4', 'total_total_tn_lag_5', 'total_total_tn_diff_5', 'total_total_tn_lag_6', 'total_total_tn_diff_6', 'total_total_tn_lag_7', 'total_total_tn_diff_7', 'total_total_tn_lag_8', 'total_total_tn_diff_8', 'total_total_tn_lag_9', 'total_total_tn_diff_9', 'total_total_tn_lag_10', 'total_total_tn_diff_10', 'total_total_tn_lag_11', 'total_total_tn_diff_11', 'total_total_tn_lag_12', 'total_total_tn_diff_12', 'total_total_tn_lag_13', 'total_total_tn_diff_13', 'total_total_tn_ma_3', 'total_total_tn_ma_6', 'total_total_tn_ma_12', 'total_total_tn_min_3', 'total_total_tn_min_6', 'total_total_tn_min_12', 'total_total_tn_std_3', 'total_total_tn_std_6', 'total_total_tn_std_12', 'total_total_delta_media_movil_3', 'total_total_delta_media_movil_6', 'total_total_delta_media_movil_12', 'total_total_delta_min_movil_3', 'total_total_delta_min_movil_6', 'total_total_delta_min_movil_12', 'total_total_delta_std_movil_3', 'total_total_delta_std_movil_', 'total_total_delta_std_movil_12', 'total_trend', 'total_season_yearly', 'clase_producto', 'producto_avg_tn', 'producto_std_tn', 'producto_min_tn', 'producto_max_tn', 'producto_clientes_distintos', 'producto_tn_media_movil_3(con_mes_en_curso)', 'producto_tn_media_movil_3anteriores', 'producto_crecimiento_ventas_suavizado', 'producto_clientes_distintos_lag_1', 'producto_clientes_distintos_growth_1', 'prod_trend', 'prod_season_yearly', 'cat_total_tn', 'cat_avg_tn', 'cat_std_tn', 'cat_min_tn', 'cat_max_tn', 'cat_productos_distintos', 'cat_total_tn_lag_1', 'cat_delta_tn_lag_1', 'cat_total_tn_lag_2', 'cat_delta_tn_lag_2', 'cat_total_tn_lag_3', 'cat_delta_tn_lag_3', 'cat_total_tn_lag_4', 'cat_delta_tn_lag_4', 'cat_total_tn_lag_5', 'cat_delta_tn_lag_5', 'cat_total_tn_lag_6', 'cat_delta_tn_lag_6', 'cat_total_tn_lag_7', 'cat_delta_tn_lag_7', 'cat_total_tn_lag_8', 'cat_delta_tn_lag_8', 'cat_total_tn_lag_9', 'cat_delta_tn_lag_9', 'cat_total_tn_lag_10', 'cat_delta_tn_lag_10', 'cat_total_tn_lag_11', 'cat_delta_tn_lag_11', 'cat_total_tn_lag_12', 'cat_delta_tn_lag_12', 'cat_total_tn_lag_13', 'cat_delta_tn_lag_13', 'cat_total_tn_ma_3', 'cat_total_tn_ma_6', 'cat_total_tn_ma_12', 'cat_total_tn_min_3', 'cat_total_tn_min_6', 'cat_total_tn_min_12', 'cat_total_tn_std_3', 'cat_total_tn_std_6', 'cat_total_tn_std_12', 'cat_total_delta_media_movil_3', 'cat_total_delta_media_movil_6', 'cat_total_delta_media_movil_12', 'cat_total_delta_min_movil_3', 'cat_total_delta_min_movil_6', 'cat_total_delta_min_movil_12', 'cat_total_delta_std_movil_3', 'cat_total_delta_std_movil_6', 'cat_total_delta_std_movil_12', 'cat_trend', 'cat_season_yearly', 'share_producto_en_categoria', 'share_producto_en_categoria_lag_1', 'share_producto_en_categoria_lag_2', 'share_producto_en_categoria_lag_3', 'share_producto_en_categoria_lag_4', 'share_producto_en_categoria_lag_5', 'share_producto_en_categoria_lag_6', 'share_producto_en_categoria_lag_7', 'share_producto_en_categoria_lag_8', 'share_producto_en_categoria_lag_9', 'share_producto_en_categoria_lag_10', 'share_producto_en_categoria_lag_11', 'share_producto_en_categoria_lag_12', 'share_producto_en_categoria_lag_13', 'tasa_crecimiento_share_producto_en_categoria', 'tasa_crecimiento_share_producto_en_categoria_lag_1', 'tasa_crecimiento_share_producto_en_categoria_lag_2', 'tasa_crecimiento_share_producto_en_categoria_lag_3', 'tasa_crecimiento_share_producto_en_categoria_lag_4', 'tasa_crecimiento_share_producto_en_categoria_lag_5', 'tasa_crecimiento_share_producto_en_categoria_lag_6', 'tasa_crecimiento_share_producto_en_categoria_lag_7', 'tasa_crecimiento_share_producto_en_categoria_lag_8', 'tasa_crecimiento_share_producto_en_categoria_lag_9', 'tasa_crecimiento_share_producto_en_categoria_lag_10', 'tasa_crecimiento_share_producto_en_categoria_lag_11', 'tasa_crecimiento_share_producto_en_categoria_lag_12', 'tasa_crecimiento_share_producto_en_categoria_lag_13', 'share_producto_en_categoria_movil_3(con_mes_en_curso)', 'share_producto_en_categoria_3anteriores', 'share_producto_en_categoria_suavizado', 'meses_vida_producto', 'otros_total_tn_', 'otros_total_tn_lag1', 'otros_total_tn_lag2', 'otros_total_tn_lag3', 'otros_total_tn_lag4', 'otros_total_tn_lag5', 'otros_total_tn_lag6', 'otros_total_tn_lag7', 'otros_total_tn_lag8', 'otros_total_tn_lag9', 'otros_total_tn_lag10', 'otros_total_tn_lag11', 'otros_total_tn_lag12', 'otros_total_tn_lag13', 'otros_avg', 'otros_avg_lag1', 'otros_avg_lag2', 'otros_avg_lag3', 'otros_avg_lag4', 'otros_avg_lag5', 'otros_avg_lag6', 'otros_avg_lag7', 'otros_avg_lag8', 'otros_avg_lag9', 'otros_avg_lag10', 'otros_avg_lag11', 'otros_avg_lag12', 'otros_avg_lag13', 'tn_pred_auto', 'n_meses_hist', 'tn_pred_auto_delta_a_tn', 'ratio_tn_pred_auto_delta_a_tn', 'ratio_tn_pred_a_tn', 'cluster_2', 'cluster_3', 'cluster_10', 'cluster_50', 'cluster_100', 'cluster_500']\n"
     ]
    }
   ],
   "source": [
    "numeric_feats = dataset.select_dtypes(include=['number']).columns.tolist()\n",
    "print(numeric_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29652, 311)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3074"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        #   - Datetime\n",
    "        # 1) Datetime → int64 (ns) → float32\n",
    "        if pd.api.types.is_datetime64_any_dtype(col_type):\n",
    "            # view() extrae los nanosegundos desde epoch\n",
    "            df[col] = df[col].view('int64').astype('float32')\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Solo nos ocupamos de numéricos\n",
    "        if not pd.api.types.is_numeric_dtype(col_type):\n",
    "            if not pd.api.types.is_categorical_dtype(col_type):\n",
    "                df[col] = df[col].astype('category')\n",
    "            continue\n",
    "\n",
    "        c_min, c_max = df[col].min(), df[col].max()\n",
    "        has_na = df[col].isnull().any()\n",
    "\n",
    "        # --- ENTEROS ---\n",
    "        if pd.api.types.is_integer_dtype(col_type):\n",
    "            # 1) Sin nulos → numpy ints\n",
    "            if not has_na:\n",
    "                if c_min >= np.iinfo(np.int8).min  and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "\n",
    "\n",
    "        # --- FLOTANTES ---\n",
    "        else:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'Uso de memoria inicial del DataFrame: {start_mem:.2f} MB')\n",
    "        print(f'Uso de memoria final del DataFrame:   {end_mem:.2f} MB')\n",
    "        print(f'Memoria reducida en un {(100*(start_mem-end_mem)/start_mem):.2f}%')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria inicial del DataFrame: 19.68 MB\n",
      "Uso de memoria final del DataFrame:   33.22 MB\n",
      "Memoria reducida en un -68.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\4128757994.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(col_type):\n",
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\4128757994.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(col_type):\n",
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\4128757994.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(col_type):\n",
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\4128757994.py:11: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df[col] = df[col].view('int64').astype('float32')\n"
     ]
    }
   ],
   "source": [
    "dataset = reduce_mem_usage(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29652, 311)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset,\n",
    "                               test_size = TEST_SIZE,\n",
    "                               random_state = SEED,\n",
    "                               ) #stratify = dataset.clase saco esto porque no es un problema de clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23721, 311)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar el DataFrame\n",
    "del dataset\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'clase_producto'\n",
    "features = [col for col in train.columns if col != label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(\"train_tmp.parquet\", index=False)\n",
    "del train\n",
    "gc.collect()\n",
    "\n",
    "train = pd.read_parquet(\"train_tmp.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['producto_total_tn'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diana\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producto_total_tn      float64\n",
      "avg_tn                 float64\n",
      "std_tn                 float64\n",
      "clientes_distintos     float64\n",
      "cust_request_qty       float64\n",
      "                        ...   \n",
      "cat2                  category\n",
      "cat3                  category\n",
      "brand                 category\n",
      "categoria             category\n",
      "estado_producto       category\n",
      "Length: 311, dtype: object\n",
      "   producto_total_tn    avg_tn    std_tn  clientes_distintos  \\\n",
      "0           0.139069  0.141088  0.177842           17.227564   \n",
      "1           0.727734  0.762872  0.652542           16.673937   \n",
      "2           0.243405  0.255158  0.241928           16.673937   \n",
      "3           0.078218  0.073109  0.072461           18.693046   \n",
      "4           0.045483  0.046849  0.034551           16.967033   \n",
      "\n",
      "   cust_request_qty  cust_request_tn  inicio_vida_p   fin_vida_p  sku_size  \\\n",
      "0          1.440458         0.134560    4965.009136  6259.601069  0.467223   \n",
      "1          2.480789         0.704141    4964.787604  6262.888968  0.408820   \n",
      "2          1.928613         0.235780    4964.787604  6262.888968  0.759237   \n",
      "3          0.704224         0.075682    4964.787604  6262.888968  0.554827   \n",
      "4          2.360751         0.044009    4964.787604  6262.733879  0.408820   \n",
      "\n",
      "   stock_final  ...  cluster_500  periodo  product_id  clase_producto   cat1  \\\n",
      "0          NaN  ...     0.012872   201801       20748       27.265625     PC   \n",
      "1          NaN  ...     2.066020   201704       20194       -7.812500  FOODS   \n",
      "2          NaN  ...     2.117510   201704       20295       -1.812500     PC   \n",
      "3     0.367741  ...     1.615486   201906       20399        6.609375  FOODS   \n",
      "4          NaN  ...     0.624311   201708       20776       -0.539062     PC   \n",
      "\n",
      "             cat2             cat3     brand                        categoria  \\\n",
      "0         CABELLO      Acond Mujer     NIVEA           PC_CABELLO_Acond Mujer   \n",
      "1  SOPAS Y CALDOS       Salsas Wet     MAGGI  FOODS_SOPAS Y CALDOS_Salsas Wet   \n",
      "2         CABELLO          SHAMPOO  SHAMPOO3               PC_CABELLO_SHAMPOO   \n",
      "3        ADEREZOS         Mayonesa    NATURA          FOODS_ADEREZOS_Mayonesa   \n",
      "4         CABELLO  Tratamiento Fem     NIVEA       PC_CABELLO_Tratamiento Fem   \n",
      "\n",
      "   estado_producto  \n",
      "0      crecimiento  \n",
      "1      crecimiento  \n",
      "2      contraccion  \n",
      "3      contraccion  \n",
      "4      contraccion  \n",
      "\n",
      "[5 rows x 311 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ⚙️ definís tu scaler\n",
    "class StdDivisorScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X)\n",
    "        self.stds_ = np.nanstd(X, axis=0)\n",
    "        self.stds_[self.stds_ == 0] = 1.0\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X)\n",
    "        X_out = X.copy()\n",
    "        for col_idx in range(X_out.shape[1]):\n",
    "            col_data = X_out[:, col_idx]\n",
    "            mask = ~np.isnan(col_data)\n",
    "            col_data[mask] = col_data[mask] / self.stds_[col_idx]\n",
    "            X_out[:, col_idx] = col_data\n",
    "        return X_out\n",
    "\n",
    "# columnas\n",
    "cols_to_keep = ['periodo', 'product_id', 'clase_producto'] + char_feats\n",
    "cols_to_scale = [col for col in train.columns if col not in cols_to_keep]\n",
    "\n",
    "# dtypes originales\n",
    "dtypes_originales = train[cols_to_keep].dtypes.to_dict()\n",
    "dtypes_escala = train[cols_to_scale].dtypes.to_dict()\n",
    "\n",
    "# 🔷 aquí creás y fiteás el preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('custom_scaler', StdDivisorScaler(), cols_to_scale),\n",
    "        ('passthrough', 'passthrough', cols_to_keep)\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor.fit(train)\n",
    "\n",
    "# Ahora sí, podés usar el scaler individualmente\n",
    "X_scaled = preprocessor.named_transformers_['custom_scaler'].transform(train[cols_to_scale])\n",
    "\n",
    "scaled_df = pd.DataFrame(X_scaled, columns=cols_to_scale, index=train.index)\n",
    "\n",
    "for col in cols_to_scale:\n",
    "    if pd.api.types.is_integer_dtype(dtypes_escala[col]):\n",
    "        scaled_df[col] = scaled_df[col].astype(float)\n",
    "\n",
    "passthrough_df = train[cols_to_keep].copy()\n",
    "for col in cols_to_keep:\n",
    "    passthrough_df[col] = passthrough_df[col].astype(dtypes_originales[col])\n",
    "\n",
    "train = pd.concat([scaled_df, passthrough_df], axis=1)\n",
    "\n",
    "print(train.dtypes)\n",
    "print(train.head())\n",
    "\n",
    "# opcional: guardar todo\n",
    "joblib.dump((preprocessor, cols_to_scale, cols_to_keep, dtypes_originales, dtypes_escala), 'escalador_completo_stddiv.joblib')\n",
    "\n",
    "del X_scaled\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['clase_producto'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_parquet(\"test_tmp.parquet\", index=False)\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "test = pd.read_parquet(\"test_tmp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ DataFrame final TEST con std divisor scaler:\n",
      "\n",
      "producto_total_tn      float64\n",
      "avg_tn                 float64\n",
      "std_tn                 float64\n",
      "clientes_distintos     float64\n",
      "cust_request_qty       float64\n",
      "                        ...   \n",
      "cat2                  category\n",
      "cat3                  category\n",
      "brand                 category\n",
      "categoria             category\n",
      "estado_producto       category\n",
      "Length: 311, dtype: object\n",
      "   producto_total_tn    avg_tn    std_tn  clientes_distintos  \\\n",
      "0           0.008812  0.008344  0.006130           18.465082   \n",
      "1           0.004923  0.005129  0.004617           16.771636   \n",
      "2           0.018745  0.018873  0.043925           17.357829   \n",
      "3           0.241062  0.246344  0.289846           17.097299   \n",
      "4           0.005793  0.005545  0.006115           18.269685   \n",
      "\n",
      "   cust_request_qty  cust_request_tn  inicio_vida_p   fin_vida_p  sku_size  \\\n",
      "0          1.088346         0.008527    4964.787604  6262.888968  0.233611   \n",
      "1          0.776247         0.004763    4964.787604  6262.888968  0.233611   \n",
      "2          0.552176         0.018137    4964.787604  6259.694123  0.011681   \n",
      "3          1.552494         0.233247    4964.787604  6262.888968  1.051251   \n",
      "4          0.656209         0.005605    4967.470595  6262.888968  0.292014   \n",
      "\n",
      "   stock_final  ...  cluster_500  periodo  product_id  clase_producto   cat1  \\\n",
      "0     0.117008  ...     0.180214   201903       20977       -0.242188     PC   \n",
      "1          NaN  ...     0.012872   201706       21077        0.141602     PC   \n",
      "2          NaN  ...     1.364474   201804       21045       -2.091797  FOODS   \n",
      "3          NaN  ...     0.238139   201710       20280      -12.914062     HC   \n",
      "4     0.077981  ...     0.180214   201902       21001       -0.148438     PC   \n",
      "\n",
      "             cat2                 cat3     brand  \\\n",
      "0         CABELLO      Tratamiento Fem     NIVEA   \n",
      "1         CABELLO            POST WASH  SHAMPOO1   \n",
      "2  SOPAS Y CALDOS          Sazonadores     MAGGI   \n",
      "3     ROPA LAVADO              Liquido   LIMPIEX   \n",
      "4           PIEL2  Jabon Antibacterial     DEOS1   \n",
      "\n",
      "                          categoria  estado_producto  \n",
      "0        PC_CABELLO_Tratamiento Fem      crecimiento  \n",
      "1              PC_CABELLO_POST WASH      crecimiento  \n",
      "2  FOODS_SOPAS Y CALDOS_Sazonadores          estable  \n",
      "3            HC_ROPA LAVADO_Liquido          estable  \n",
      "4      PC_PIEL2_Jabon Antibacterial      contraccion  \n",
      "\n",
      "[5 rows x 311 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\4039145587.py:27: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(dtypes_originales[col]) or pd.api.types.is_object_dtype(dtypes_originales[col]):\n",
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\4039145587.py:27: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(dtypes_originales[col]) or pd.api.types.is_object_dtype(dtypes_originales[col]):\n",
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\4039145587.py:27: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(dtypes_originales[col]) or pd.api.types.is_object_dtype(dtypes_originales[col]):\n",
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\4039145587.py:27: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(dtypes_originales[col]) or pd.api.types.is_object_dtype(dtypes_originales[col]):\n",
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\4039145587.py:27: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(dtypes_originales[col]) or pd.api.types.is_object_dtype(dtypes_originales[col]):\n",
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\4039145587.py:27: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(dtypes_originales[col]) or pd.api.types.is_object_dtype(dtypes_originales[col]):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🔷 Cargar lo que guardaste en train\n",
    "(preprocessor, cols_to_scale, cols_to_keep, dtypes_originales, dtypes_escala) = joblib.load(\n",
    "    'escalador_completo_stddiv.joblib'\n",
    ")\n",
    "\n",
    "# 🔷 Transformar todo test\n",
    "X_test_transformed = preprocessor.transform(test)\n",
    "\n",
    "# El resultado es un numpy.ndarray con las columnas en el orden:\n",
    "# primero cols_to_scale (escaladas) y después cols_to_keep (passthrough)\n",
    "# reconstruimos en DataFrame\n",
    "all_cols_order = cols_to_scale + cols_to_keep\n",
    "test = pd.DataFrame(X_test_transformed, columns=all_cols_order, index=test.index)\n",
    "\n",
    "# 🔷 Restaurar dtypes en cols_to_scale y cols_to_keep\n",
    "\n",
    "# Para cols_to_scale: asegurarse de que las originalmente int sigan siendo float (por los NaN)\n",
    "for col in cols_to_scale:\n",
    "    if pd.api.types.is_integer_dtype(dtypes_escala[col]):\n",
    "        test[col] = pd.to_numeric(test[col], errors='coerce').astype(float)\n",
    "    elif pd.api.types.is_float_dtype(dtypes_escala[col]):\n",
    "        test[col] = pd.to_numeric(test[col], errors='coerce').astype(float)\n",
    "\n",
    "# Para cols_to_keep: devolver exactamente al dtype original\n",
    "for col in cols_to_keep:\n",
    "    # primero asegurarse que esté en str si es necesario\n",
    "    if pd.api.types.is_categorical_dtype(dtypes_originales[col]) or pd.api.types.is_object_dtype(dtypes_originales[col]):\n",
    "        test[col] = test[col].astype(str)\n",
    "    # ahora castear al tipo original\n",
    "    test[col] = test[col].astype(dtypes_originales[col])\n",
    "\n",
    "print(\"\\n✅ DataFrame final TEST con std divisor scaler:\\n\")\n",
    "print(test.dtypes)\n",
    "print(test.head())\n",
    "\n",
    "# 🔷 liberar memoria\n",
    "del X_test_transformed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23721, 311)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[features]\n",
    "y_train = train[label]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23721, 310)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "#del test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (23721, 310)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial):\n",
    "\n",
    "    # Parámetros para LightGBM\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse', # Cambiado a Mean Squared Error\n",
    "        'verbosity': -1,\n",
    "        'linear_tree': True,\n",
    "        'seed': SEED,\n",
    "        # 'num_class': len(y_train.unique()), # Eliminar, esto es para clasificación\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 150),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 0, 200),\n",
    "        'max_bin': trial.suggest_int('max_bin', 64, 1024),\n",
    "        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-3, 10.0, log=True)\n",
    "    }\n",
    "        \n",
    "    # Voy a generar estimaciones de los 5 modelos del CV sobre los datos test y los acumulo en la matriz scores_ensemble\n",
    "    # Para regresión, scores_ensemble será un array 1D\n",
    "    scores_ensemble = np.zeros(len(y_test),dtype=np.float32)\n",
    "\n",
    "    # Score del 5 fold CV inicializado en 0\n",
    "    score_folds = 0\n",
    "\n",
    "    # Numero de splits del CV\n",
    "    n_splits = 5\n",
    "\n",
    "    # Objeto para hacer el split de CV (usar KFold para regresión, ya que StratifiedKFold es para clasificación)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED) # Added shuffle and random_state for reproducibility\n",
    "    best_iterations = []  # ⬅️ para acumular las best_iteration de cada fold\n",
    "\n",
    "    for i, (if_index, oof_index) in enumerate(kf.split(X_train, y_train)): # Usar kf.split\n",
    "\n",
    "        # Dataset in fold (donde entreno)\n",
    "        lgb_if_dataset = lgb.Dataset(data=X_train.iloc[if_index],\n",
    "                                     label=y_train.iloc[if_index],\n",
    "                                     free_raw_data=True, categorical_feature=char_feats) #cambie free raw data a true\n",
    "\n",
    "        # Dataset Out of fold (donde mido la performance del CV)\n",
    "        lgb_oof_dataset = lgb.Dataset(data=X_train.iloc[oof_index],\n",
    "                                      label=y_train.iloc[oof_index],\n",
    "                                      free_raw_data=True, categorical_feature=char_feats) #cambie free raw data a true\n",
    "\n",
    "        # Entreno el modelo\n",
    "        lgb_model = lgb.train(lgb_params,\n",
    "                              lgb_if_dataset,\n",
    "                              valid_sets=lgb_oof_dataset,\n",
    "                              num_boost_round=10000,\n",
    "                              callbacks=[lgb.early_stopping(200, verbose=False)],\n",
    "                              # feval = mean_squared_error(y_test, preds, squared=False) # Eliminar o definir correctamente para custom metric\n",
    "                             )\n",
    "\n",
    "        # Acumulo las predicciones continuas para el conjunto de test\n",
    "        scores_ensemble = scores_ensemble + lgb_model.predict(X_test)\n",
    "\n",
    "        # Score del fold (registros de dataset train que en este fold quedan out of fold)\n",
    "        # Calcular MSE para el fold OOF\n",
    "        oof_preds = lgb_model.predict(X_train.iloc[oof_index])\n",
    "        score_folds += mean_squared_error(y_train.iloc[oof_index], oof_preds) / n_splits\n",
    "        \n",
    "        # guardo el best_iteration de este fold\n",
    "        best_iterations.append(lgb_model.best_iteration)\n",
    "        \n",
    "        # ⬇️ Liberar memoria de objetos pesados del fold\n",
    "        del lgb_model, lgb_if_dataset, lgb_oof_dataset, oof_preds\n",
    "        gc.collect()\n",
    "       #print(f\"fin split {i}\")\n",
    "\n",
    "    # Promedio las predicciones del ensemble para el conjunto de test\n",
    "    scores_ensemble = scores_ensemble / n_splits\n",
    "\n",
    "    # Guardo prediccion del trial sobre el conjunto de test\n",
    "    # Genero nombre de archivo\n",
    "    #predicted_filename = os.path.join(PATH_TO_TEMP_FILES,f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    # Copia del dataset para guardar la prediccion\n",
    "    #predicted_df = test.copy()\n",
    "    #predicted_df= pd.DataFrame({\n",
    "    #'periodo': test['periodo'],\n",
    "    #'customer_id': test['customer_id'],  # o el ID que necesites\n",
    "    #'product_id' : test['product_id'],\n",
    "    #'pred': scores_ensemble\n",
    "    #})\n",
    "    \n",
    "    # Genero columna pred con predicciones promediadas de los 5 folds\n",
    "    #predicted_df['pred'] = scores_ensemble\n",
    "    # Grabo dataframe en temp_artifacts\n",
    "    #dump(predicted_df, predicted_filename)\n",
    "    # Indico a optuna que asocie el archivo generado al trial\n",
    "    #upload_artifact(trial, predicted_filename, artifact_store)\n",
    "    #del predicted_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Grabo métricas de regresión en lugar de matriz de confusión\n",
    "    # Puedes guardar métricas como MSE, RMSE, MAE, R2, etc. en un archivo de texto o log\n",
    "    # Por ejemplo, calcular el MSE en el conjunto de test\n",
    "    test_mse = mean_squared_error(y_test, scores_ensemble)\n",
    "    trial.set_user_attr(\"test_mse\", test_mse) # Almacenar MSE del test como atributo de usuario\n",
    "\n",
    "    # guardar el promedio de las mejores iteraciones\n",
    "    avg_best_iteration = int(np.mean(best_iterations))\n",
    "    trial.set_user_attr(\"best_iteration\", avg_best_iteration)\n",
    "    \n",
    "    del scores_ensemble\n",
    "    gc.collect()\n",
    "\n",
    "    # Si quieres una visualización, podrías generar un scatter plot de predicciones vs valores reales\n",
    "    # O un histograma de residuos. Aquí un placeholder para un \"regression_metrics.txt\"\n",
    "    #regression_metrics_filename = os.path.join(PATH_TO_TEMP_FILES, f'regression_metrics_{trial.study.study_name}_{trial.number}.txt')\n",
    "    #with open(regression_metrics_filename, 'w') as f:\n",
    "    #    f.write(f\"Test Mean Squared Error: {test_mse}\\n\")\n",
    "    #    # Add other regression metrics if desired\n",
    "    #upload_artifact(trial, regression_metrics_filename, artifact_store)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    # Devuelvo el score promedio de MSE del 5-fold CV a Optuna para que optimice en base a eso\n",
    "    # Optuna minimiza por defecto, por lo que devolver MSE es apropiado.\n",
    "    return score_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-17 12:53:06,957] Using an existing study with name 'producto_linear' instead of creating a new one.\n",
      "[I 2025-07-17 12:57:55,431] Trial 268 finished with value: 891.8910174659568 and parameters: {'lambda_l1': 2.325916090763849, 'lambda_l2': 0.0001437194737808914, 'num_leaves': 148, 'feature_fraction': 0.9687817423298575, 'learning_rate': 0.01887976400286657, 'bagging_fraction': 0.9379939595657532, 'bagging_freq': 1, 'min_child_samples': 189, 'max_bin': 946, 'min_sum_hessian_in_leaf': 0.04159730162210393}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 13:04:15,261] Trial 269 finished with value: 900.3588286581573 and parameters: {'lambda_l1': 1.5215125164342471, 'lambda_l2': 0.00025443723673679894, 'num_leaves': 147, 'feature_fraction': 0.9842392608520609, 'learning_rate': 0.014338406476159535, 'bagging_fraction': 0.9264616889088194, 'bagging_freq': 1, 'min_child_samples': 149, 'max_bin': 849, 'min_sum_hessian_in_leaf': 0.051951256409332104}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 13:06:51,800] Trial 270 finished with value: 934.9146442231786 and parameters: {'lambda_l1': 3.3183513159982043, 'lambda_l2': 0.0007095714777875667, 'num_leaves': 148, 'feature_fraction': 0.9988394977893243, 'learning_rate': 0.08515071493224685, 'bagging_fraction': 0.9477974866823908, 'bagging_freq': 1, 'min_child_samples': 82, 'max_bin': 971, 'min_sum_hessian_in_leaf': 0.07223919274394885}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 13:14:22,928] Trial 271 finished with value: 888.1773862266152 and parameters: {'lambda_l1': 0.3414385857752815, 'lambda_l2': 8.283425174144145e-05, 'num_leaves': 149, 'feature_fraction': 0.9758143297217238, 'learning_rate': 0.010185958052969724, 'bagging_fraction': 0.912957902896447, 'bagging_freq': 1, 'min_child_samples': 161, 'max_bin': 820, 'min_sum_hessian_in_leaf': 0.08443010664712826}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 13:18:41,703] Trial 272 finished with value: 898.3321592547537 and parameters: {'lambda_l1': 6.35014423235445, 'lambda_l2': 0.00015574790839221458, 'num_leaves': 149, 'feature_fraction': 0.9595832862810012, 'learning_rate': 0.016834234477978195, 'bagging_fraction': 0.9269469926322202, 'bagging_freq': 1, 'min_child_samples': 178, 'max_bin': 893, 'min_sum_hessian_in_leaf': 0.06460829765588816}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 13:27:22,753] Trial 273 finished with value: 894.9377091146016 and parameters: {'lambda_l1': 0.7848866056702983, 'lambda_l2': 0.0003590098524667376, 'num_leaves': 139, 'feature_fraction': 0.9422894612870711, 'learning_rate': 0.013180912471351239, 'bagging_fraction': 0.9176960368887281, 'bagging_freq': 1, 'min_child_samples': 103, 'max_bin': 1010, 'min_sum_hessian_in_leaf': 0.02444392184844765}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 13:34:07,118] Trial 274 finished with value: 892.2291823657164 and parameters: {'lambda_l1': 1.8348606478985292, 'lambda_l2': 0.0005072564366547975, 'num_leaves': 149, 'feature_fraction': 0.9891126494356512, 'learning_rate': 0.015276483431226933, 'bagging_fraction': 0.9348515783259492, 'bagging_freq': 1, 'min_child_samples': 142, 'max_bin': 920, 'min_sum_hessian_in_leaf': 0.04897745909878601}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 13:40:21,308] Trial 275 finished with value: 892.1769980862115 and parameters: {'lambda_l1': 4.6710211801366865, 'lambda_l2': 0.00010427579869298355, 'num_leaves': 149, 'feature_fraction': 0.999731526656311, 'learning_rate': 0.013003540871621068, 'bagging_fraction': 0.9045225113381506, 'bagging_freq': 1, 'min_child_samples': 103, 'max_bin': 947, 'min_sum_hessian_in_leaf': 0.037600638642573424}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 13:43:05,791] Trial 276 finished with value: 925.5292073784285 and parameters: {'lambda_l1': 2.5753716385820904, 'lambda_l2': 0.00021978001354472216, 'num_leaves': 143, 'feature_fraction': 0.5279129115497373, 'learning_rate': 0.018147211951743368, 'bagging_fraction': 0.9520953274483337, 'bagging_freq': 1, 'min_child_samples': 173, 'max_bin': 859, 'min_sum_hessian_in_leaf': 0.05867054124408504}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 13:50:05,198] Trial 277 finished with value: 877.5152867819368 and parameters: {'lambda_l1': 1.327038805212501, 'lambda_l2': 6.920518798256715e-05, 'num_leaves': 138, 'feature_fraction': 0.9712355312631449, 'learning_rate': 0.014971399009115564, 'bagging_fraction': 0.9417414412679639, 'bagging_freq': 4, 'min_child_samples': 133, 'max_bin': 930, 'min_sum_hessian_in_leaf': 0.04558063222109418}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 13:56:31,213] Trial 278 finished with value: 892.1805660479234 and parameters: {'lambda_l1': 3.608930788090106, 'lambda_l2': 7.228289199168883e-05, 'num_leaves': 130, 'feature_fraction': 0.953746664434556, 'learning_rate': 0.01010303563746745, 'bagging_fraction': 0.9438267785552968, 'bagging_freq': 4, 'min_child_samples': 86, 'max_bin': 899, 'min_sum_hessian_in_leaf': 0.019869192852612794}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 14:00:11,306] Trial 279 finished with value: 892.7168489154985 and parameters: {'lambda_l1': 1.4197775548319806, 'lambda_l2': 6.347385128011551e-05, 'num_leaves': 149, 'feature_fraction': 0.9674172474421447, 'learning_rate': 0.019897947740627475, 'bagging_fraction': 0.9539342260626824, 'bagging_freq': 4, 'min_child_samples': 125, 'max_bin': 839, 'min_sum_hessian_in_leaf': 0.04110411683386635}. Best is trial 135 with value: 872.4531699475713.\n",
      "[I 2025-07-17 14:04:15,521] Trial 280 finished with value: 926.0532881599506 and parameters: {'lambda_l1': 2.124626307008747, 'lambda_l2': 4.476385055544528e-05, 'num_leaves': 149, 'feature_fraction': 0.9830827933919375, 'learning_rate': 0.016279767964696178, 'bagging_fraction': 0.8086771257189929, 'bagging_freq': 4, 'min_child_samples': 121, 'max_bin': 879, 'min_sum_hessian_in_leaf': 0.031203400162516933}. Best is trial 135 with value: 872.4531699475713.\n",
      "[W 2025-07-17 14:05:41,485] Trial 281 failed with parameters: {'lambda_l1': 0.5479125738544504, 'lambda_l2': 0.00011655970324724096, 'num_leaves': 149, 'feature_fraction': 0.7964995666903733, 'learning_rate': 0.014155927803576052, 'bagging_fraction': 0.96040740974362, 'bagging_freq': 4, 'min_child_samples': 199, 'max_bin': 926, 'min_sum_hessian_in_leaf': 0.012995255668795816} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\diana\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_20280\\1239698308.py\", line 50, in lgb_objective\n",
      "    lgb_model = lgb.train(lgb_params,\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\diana\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\engine.py\", line 322, in train\n",
      "    booster.update(fobj=fobj)\n",
      "  File \"C:\\Users\\diana\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\basic.py\", line 4155, in update\n",
      "    _LIB.LGBM_BoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-17 14:05:41,488] Trial 281 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[325], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m                             storage\u001b[38;5;241m=\u001b[39mnombrebase,  \u001b[38;5;66;03m# Specify the storage URL here.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m                             study_name\u001b[38;5;241m=\u001b[39mnombreestudio,\n\u001b[0;32m      8\u001b[0m                             load_if_exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#Corro la optimizacion\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(lgb_objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, gc_after_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     _optimize(\n\u001b[0;32m    490\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    491\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    492\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    493\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    494\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    495\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    496\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    497\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    498\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    499\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         _optimize_sequential(\n\u001b[0;32m     65\u001b[0m             study,\n\u001b[0;32m     66\u001b[0m             func,\n\u001b[0;32m     67\u001b[0m             n_trials,\n\u001b[0;32m     68\u001b[0m             timeout,\n\u001b[0;32m     69\u001b[0m             catch,\n\u001b[0;32m     70\u001b[0m             callbacks,\n\u001b[0;32m     71\u001b[0m             gc_after_trial,\n\u001b[0;32m     72\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     74\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     75\u001b[0m         )\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[324], line 50\u001b[0m, in \u001b[0;36mlgb_objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     45\u001b[0m lgb_oof_dataset \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(data\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39miloc[oof_index],\n\u001b[0;32m     46\u001b[0m                               label\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39miloc[oof_index],\n\u001b[0;32m     47\u001b[0m                               free_raw_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, categorical_feature\u001b[38;5;241m=\u001b[39mchar_feats) \u001b[38;5;66;03m#cambie free raw data a true\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Entreno el modelo\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(lgb_params,\n\u001b[0;32m     51\u001b[0m                       lgb_if_dataset,\n\u001b[0;32m     52\u001b[0m                       valid_sets\u001b[38;5;241m=\u001b[39mlgb_oof_dataset,\n\u001b[0;32m     53\u001b[0m                       num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[0;32m     54\u001b[0m                       callbacks\u001b[38;5;241m=\u001b[39m[lgb\u001b[38;5;241m.\u001b[39mearly_stopping(\u001b[38;5;241m200\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)],\n\u001b[0;32m     55\u001b[0m                       \u001b[38;5;66;03m# feval = mean_squared_error(y_test, preds, squared=False) # Eliminar o definir correctamente para custom metric\u001b[39;00m\n\u001b[0;32m     56\u001b[0m                      )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Acumulo las predicciones continuas para el conjunto de test\u001b[39;00m\n\u001b[0;32m     59\u001b[0m scores_ensemble \u001b[38;5;241m=\u001b[39m scores_ensemble \u001b[38;5;241m+\u001b[39m lgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\engine.py:322\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    311\u001b[0m     cb(\n\u001b[0;32m    312\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[0;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m         )\n\u001b[0;32m    320\u001b[0m     )\n\u001b[1;32m--> 322\u001b[0m booster\u001b[38;5;241m.\u001b[39mupdate(fobj\u001b[38;5;241m=\u001b[39mfobj)\n\u001b[0;32m    324\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\lightgbm\\basic.py:4155\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   4153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4154\u001b[0m _safe_call(\n\u001b[1;32m-> 4155\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterUpdateOneIter(\n\u001b[0;32m   4156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[0;32m   4157\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(is_finished),\n\u001b[0;32m   4158\u001b[0m     )\n\u001b[0;32m   4159\u001b[0m )\n\u001b[0;32m   4160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   4161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Inicio el store de artefactos (archivos) de optuna\n",
    "artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "#Genero estudio\n",
    "study = optuna.create_study(direction='minimize',\n",
    "                            storage=nombrebase,  # Specify the storage URL here.\n",
    "                            study_name=nombreestudio,\n",
    "                            load_if_exists = True)\n",
    "#Corro la optimizacion\n",
    "study.optimize(lgb_objective, n_trials=2000, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar modelos con mejores parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar el trial con menor mse_test en los atributos de usuario\n",
    "best_trial = min(\n",
    "    [t for t in study.trials if t.user_attrs.get(\"test_mse\") is not None],\n",
    "    key=lambda t: t.user_attrs[\"test_mse\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=274, state=1, values=[892.2291823657164], datetime_start=datetime.datetime(2025, 7, 17, 13, 27, 23, 271155), datetime_complete=datetime.datetime(2025, 7, 17, 13, 34, 7, 85281), params={'lambda_l1': 1.8348606478985292, 'lambda_l2': 0.0005072564366547975, 'num_leaves': 149, 'feature_fraction': 0.9891126494356512, 'learning_rate': 0.015276483431226933, 'bagging_fraction': 0.9348515783259492, 'bagging_freq': 1, 'min_child_samples': 142, 'max_bin': 920, 'min_sum_hessian_in_leaf': 0.04897745909878601}, user_attrs={'best_iteration': 826, 'test_mse': 872.5765521635892}, system_attrs={}, intermediate_values={}, distributions={'lambda_l1': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'lambda_l2': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'num_leaves': IntDistribution(high=150, log=False, low=10, step=1), 'feature_fraction': FloatDistribution(high=1.0, log=False, low=0.4, step=None), 'learning_rate': FloatDistribution(high=0.1, log=False, low=0.01, step=None), 'bagging_fraction': FloatDistribution(high=1.0, log=False, low=0.5, step=None), 'bagging_freq': IntDistribution(high=7, log=False, low=1, step=1), 'min_child_samples': IntDistribution(high=200, log=False, low=0, step=1), 'max_bin': IntDistribution(high=1024, log=False, low=64, step=1), 'min_sum_hessian_in_leaf': FloatDistribution(high=10.0, log=True, low=0.001, step=None)}, trial_id=275, value=None)\n"
     ]
    }
   ],
   "source": [
    "print(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_trial.params.copy()\n",
    "best_params.update({\n",
    "    \"objective\": \"regression\",#los parametros que no se optimizan (los fijos) hay que escribirlos\n",
    "    \"metric\": \"mse\",\n",
    "    \"verbosity\": -1,\n",
    "    'seed': SEED,\n",
    "    'linear_tree': True,    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda_l1': 0.043301223417331454, 'lambda_l2': 3.3597498505196954e-06, 'num_leaves': 20, 'feature_fraction': 0.9578449292753943, 'learning_rate': 0.03626001394797111, 'bagging_fraction': 0.7840929446849241, 'bagging_freq': 1, 'min_child_samples': 11, 'max_bin': 418}\n"
     ]
    }
   ],
   "source": [
    "print( best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda_l1': 0.043301223417331454, 'lambda_l2': 3.3597498505196954e-06, 'num_leaves': 20, 'feature_fraction': 0.9578449292753943, 'learning_rate': 0.03626001394797111, 'bagging_fraction': 0.7840929446849241, 'bagging_freq': 1, 'min_child_samples': 11, 'max_bin': 418}\n"
     ]
    }
   ],
   "source": [
    "best_iteration = best_trial.user_attrs[\"best_iteration\"]\n",
    "print( best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_lgbm_final_todo(base_params, seed):\n",
    "    \"\"\"\n",
    "    Entrena un modelo final en TODO el dataset (train+test),\n",
    "    usando los mejores hiperparámetros y un número fijo de iteraciones.\n",
    "\n",
    "    Args:\n",
    "        base_params (dict): hiperparámetros óptimos (Optuna).\n",
    "        seed (int): semilla para reproducibilidad.\n",
    "\n",
    "    Returns:\n",
    "        modelo final entrenado.\n",
    "    \"\"\"\n",
    "    run_params = base_params.copy()\n",
    "    run_params.update({'seed': seed})\n",
    "\n",
    "    print(\"\\n🚀 Entrenando modelo final en TODO el dataset...\")\n",
    "\n",
    "\n",
    "    all_dataset = lgb.Dataset(\n",
    "        data=X_train,\n",
    "        label=y_train,\n",
    "        free_raw_data=True,\n",
    "        categorical_feature=char_feats\n",
    "    )\n",
    "\n",
    "    # entrenar con el mejor número de iteraciones conocido\n",
    "    # si no lo conocés, podés poner 10000, pero sin early stopping (porque no hay validación)\n",
    "    final_model = lgb.train(\n",
    "        run_params,\n",
    "        train_set=all_dataset,\n",
    "        num_boost_round=best_iteration  # 👈 poné aquí el mejor obtenido antes\n",
    "    )\n",
    "\n",
    "    # Guardar modelo\n",
    "    model_path = os.path.join(PATH_TO_MODELS, f\"lgb_final_model_todo_seed_{seed}.joblib\")\n",
    "    dump(final_model, model_path)\n",
    "    print(f\"✅ Modelo final entrenado y guardado en: {model_path}\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mover modelos previos antes del loop\n",
    "for fname in os.listdir(PATH_TO_MODELS):\n",
    "    if fname.endswith('.joblib'):\n",
    "        src_path = os.path.join(PATH_TO_MODELS, fname)\n",
    "        dst_path = os.path.join(PATH_TO_MODELS_OLD, fname)\n",
    "        shutil.move(src_path, dst_path)\n",
    "        print(f\"🔁 Modelo movido a backup: {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_42.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_5.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_915.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_15.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_666.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_9999.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_37.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_45.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_125.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_90.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_1000.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_3.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_753.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_159.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_852.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_10.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_7.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_1.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_1050.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_654.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_11.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_21.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_33.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_69.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_8008.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_88.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_111.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_222.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_314.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_420.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_512.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_777.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_808.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_999.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_1024.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_2048.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_4096.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_17.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_19.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_23.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_29.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_31.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ../LGBM/models\\lgb_final_model_todo_seed_25.joblib\n"
     ]
    }
   ],
   "source": [
    "#results = []\n",
    "SEEDS = [42,5,915,15,666,9999,37,45,125,90,1000,3,753,159,852,10,7,1,1050,654,11,21,33,69,8008,88,111,222,314,420,512,777,808,999,1024,2048,4096,17,19,23,29,31,25]\n",
    "\n",
    "for current_seed in SEEDS:\n",
    "    # <--- CORRECCIÓN 1 y 2: La función ahora se llama con los parámetros base y la semilla actual\n",
    "    entrenar_lgbm_final_todo(\n",
    "        base_params=best_params, \n",
    "        seed=current_seed\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
