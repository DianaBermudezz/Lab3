{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2277113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💬 Instalar AutoGluon si es necesario\n",
    "#%pip install autogluon.timeseries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc, os, shutil\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd6f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_safe(numerator: pd.Series, denominator: pd.Series, dtype='float32', fillna=None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calcula un porcentaje seguro como numerator / denominator.\n",
    "    - Reemplaza divisiones por cero o NaN con NaN.\n",
    "    - Opcionalmente convierte a float32.\n",
    "    - Puede rellenar NaNs con `fillna`.\n",
    "    \"\"\"\n",
    "    result = (numerator / denominator).mask((denominator == 0) | (denominator.isna()))\n",
    "    if fillna is not None:\n",
    "        result = result.fillna(fillna)\n",
    "    return result.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2370fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\"Itera por las columnas del DataFrame y modifica el tipo de datos para reducir uso de memoria.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Uso de memoria inicial del DataFrame: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(col_type):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if pd.api.types.is_integer_dtype(col_type):\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            # Sólo convertir a categoría si no lo es ya\n",
    "            if not pd.api.types.is_categorical_dtype(df[col]):\n",
    "                df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Uso de memoria final del DataFrame: {end_mem:.2f} MB')\n",
    "    print(f'Memoria reducida en un {(100 * (start_mem - end_mem) / start_mem):.2f}%')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eebc6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"cliente_producto_base.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "514946ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(items=['periodo', 'product_id', 'customer_id', 'tn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6b7bdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17173448, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8080c1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   periodo  product_id\n",
       "0   201701       20001\n",
       "1   201701       20001\n",
       "2   201701       20001\n",
       "3   201701       20001\n",
       "4   201701       20001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['periodo','product_id']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da4a14c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔷 Iteración 1/27\n",
      "   📅 Cutoff: 201708, Target: 201710\n",
      "   ✅ Series válidas: 374316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_011534'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       3.73 GB / 15.69 GB (23.7%)\n",
      "Disk Space Avail:   142.33 GB / 459.95 GB (30.9%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n",
      "Provided train_data has 2950591 rows, 374316 time series. Median time series length is 8 (min=7, max=8). \n",
      "Time series in train_data are too short for chosen num_val_windows=2. Reducing num_val_windows to 1.\n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-13 22:16:36\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.2s of the 3538.1s of remaining time.\n",
      "\t-0.9187       = Validation score (-WQL)\n",
      "\t2.53    s     = Training runtime\n",
      "\t151.08  s     = Validation (prediction) runtime\n",
      "Training timeseries model RecursiveTabular. Training for up to 282.0s of the 3384.3s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.9670       = Validation score (-WQL)\n",
      "\t4.19    s     = Training runtime\n",
      "\t5.30    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 306.8s of the 3374.5s of remaining time.\n",
      "\t-0.6276       = Validation score (-WQL)\n",
      "\t9.15    s     = Training runtime\n",
      "\t11.52   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 335.4s of the 3353.5s of remaining time.\n",
      "\t-1.0543       = Validation score (-WQL)\n",
      "\t2.13    s     = Training runtime\n",
      "\t197.67  s     = Validation (prediction) runtime\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 350.4s of the 3153.6s of remaining time.\n",
      "\t-0.8068       = Validation score (-WQL)\n",
      "\t2.20    s     = Training runtime\n",
      "\t175.17  s     = Validation (prediction) runtime\n",
      "Training timeseries model AutoETS. Training for up to 372.0s of the 2976.0s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 194050 time series (51.8%). Fallback model SeasonalNaive was used for these time series.\n",
      "\t-0.9187       = Validation score (-WQL)\n",
      "\t2.34    s     = Training runtime\n",
      "\t139.97  s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 404.8s of the 2833.5s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 471.8s of the 2830.9s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 565.8s of the 2829.2s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 742.7s of the 2828.0s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 1113.7s of the 2827.4s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 2226.8s of the 2826.8s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 0.91, 'DynamicOptimizedTheta': 0.09}\n",
      "\t-0.6250       = Validation score (-WQL)\n",
      "\t115.56  s     = Training runtime\n",
      "\t186.70  s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'WeightedEnsemble']\n",
      "Total runtime: 830.62 s\n",
      "Best model: WeightedEnsemble\n",
      "Best model score: -0.6250\n",
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1130.6 segundos.\n",
      "\n",
      "🔷 Iteración 2/27\n",
      "   📅 Cutoff: 201709, Target: 201711\n",
      "   ✅ Series válidas: 395357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_013423'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       4.37 GB / 15.69 GB (27.8%)\n",
      "Disk Space Avail:   141.60 GB / 459.95 GB (30.8%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 3466963 rows, 395357 time series. Median time series length is 9 (min=7, max=9). \n",
      "\tRemoving 68442 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 2942235 rows, 326915 time series. Median time series length is 9 (min=9, max=9). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-13 22:35:13\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 273.1s of the 3550.6s of remaining time.\n",
      "\t-0.8224       = Validation score (-WQL)\n",
      "\t131.01  s     = Training runtime\n",
      "\t129.77  s     = Validation (prediction) runtime\n",
      "Training timeseries model RecursiveTabular. Training for up to 274.1s of the 3289.6s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8804       = Validation score (-WQL)\n",
      "\t10.23   s     = Training runtime\n",
      "\t4.90    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 297.7s of the 3274.2s of remaining time.\n",
      "\t-0.6174       = Validation score (-WQL)\n",
      "\t22.58   s     = Training runtime\n",
      "\t9.69    s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 324.2s of the 3241.6s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 324.0s of the 2916.1s of remaining time.\n",
      "\t-0.7431       = Validation score (-WQL)\n",
      "\t157.46  s     = Training runtime\n",
      "\t148.87  s     = Validation (prediction) runtime\n",
      "Training timeseries model AutoETS. Training for up to 326.2s of the 2609.6s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 169640 time series (51.9%). Fallback model SeasonalNaive was used for these time series.\n",
      "\t-0.7452       = Validation score (-WQL)\n",
      "\t127.26  s     = Training runtime\n",
      "\t118.40  s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 337.7s of the 2363.7s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 393.8s of the 2363.1s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 472.3s of the 2361.7s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 590.2s of the 2360.7s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 880.1s of the 2360.1s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1759.6s of the 2359.6s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'AutoETS': 0.05, 'DirectTabular': 0.79, 'DynamicOptimizedTheta': 0.17}\n",
      "\t-0.6074       = Validation score (-WQL)\n",
      "\t180.41  s     = Training runtime\n",
      "\t276.97  s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'DynamicOptimizedTheta', 'AutoETS', 'WeightedEnsemble']\n",
      "Total runtime: 1375.99 s\n",
      "Best model: WeightedEnsemble\n",
      "Best model score: -0.6074\n",
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1822.1 segundos.\n",
      "\n",
      "🔷 Iteración 3/27\n",
      "   📅 Cutoff: 201710, Target: 201712\n",
      "   ✅ Series válidas: 403001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_020446'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       4.64 GB / 15.69 GB (29.6%)\n",
      "Disk Space Avail:   140.64 GB / 459.95 GB (30.6%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 3906043 rows, 403001 time series. Median time series length is 10 (min=7, max=10). \n",
      "\tRemoving 34384 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 3640571 rows, 368617 time series. Median time series length is 10 (min=9, max=10). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-13 23:05:35\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 273.1s of the 3550.8s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 273.0s of the 3276.1s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8485       = Validation score (-WQL)\n",
      "\t12.83   s     = Training runtime\n",
      "\t6.13    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 296.1s of the 3256.8s of remaining time.\n",
      "\t-0.6220       = Validation score (-WQL)\n",
      "\t27.25   s     = Training runtime\n",
      "\t9.77    s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 321.9s of the 3219.4s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 321.8s of the 2896.1s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 321.6s of the 2572.9s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 192468 time series (52.2%). Fallback model SeasonalNaive was used for these time series.\n",
      "\t-0.7701       = Validation score (-WQL)\n",
      "\t150.13  s     = Training runtime\n",
      "\t134.54  s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 326.8s of the 2287.9s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 381.2s of the 2287.2s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 457.2s of the 2285.8s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 571.2s of the 2284.7s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 842.0s of the 2284.0s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1683.3s of the 2283.3s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'AutoETS': 0.06, 'DirectTabular': 0.92, 'RecursiveTabular': 0.02}\n",
      "\t-0.6203       = Validation score (-WQL)\n",
      "\t127.53  s     = Training runtime\n",
      "\t150.44  s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'AutoETS', 'WeightedEnsemble']\n",
      "Total runtime: 1400.29 s\n",
      "Best model: WeightedEnsemble\n",
      "Best model score: -0.6203\n",
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1679.0 segundos.\n",
      "\n",
      "🔷 Iteración 4/27\n",
      "   📅 Cutoff: 201711, Target: 201801\n",
      "   ✅ Series válidas: 410433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_023245'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       4.23 GB / 15.69 GB (26.9%)\n",
      "Disk Space Avail:   139.79 GB / 459.95 GB (30.4%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 4348723 rows, 410433 time series. Median time series length is 11 (min=7, max=11). \n",
      "\tRemoving 21069 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 4189589 rows, 389364 time series. Median time series length is 11 (min=9, max=11). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-13 23:33:35\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 273.1s of the 3549.7s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.9s of the 3275.0s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.7870       = Validation score (-WQL)\n",
      "\t15.00   s     = Training runtime\n",
      "\t7.94    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 295.6s of the 3251.5s of remaining time.\n",
      "\t-0.6157       = Validation score (-WQL)\n",
      "\t29.37   s     = Training runtime\n",
      "\t14.31   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 320.7s of the 3207.4s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 320.6s of the 2885.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 320.4s of the 2563.2s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 22127 time series (5.7%). Fallback model SeasonalNaive was used for these time series.\n",
      "\t-0.6544       = Validation score (-WQL)\n",
      "\t154.96  s     = Training runtime\n",
      "\t144.21  s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 323.4s of the 2263.8s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 377.2s of the 2263.1s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 452.2s of the 2261.2s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 565.0s of the 2259.9s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 829.5s of the 2259.0s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1658.2s of the 2258.2s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'AutoETS': 0.28, 'DirectTabular': 0.71, 'RecursiveTabular': 0.01}\n",
      "\t-0.6053       = Validation score (-WQL)\n",
      "\t127.72  s     = Training runtime\n",
      "\t166.45  s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'AutoETS', 'WeightedEnsemble']\n",
      "Total runtime: 1425.30 s\n",
      "Best model: WeightedEnsemble\n",
      "Best model score: -0.6053\n",
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1704.4 segundos.\n",
      "\n",
      "🔷 Iteración 5/27\n",
      "   📅 Cutoff: 201712, Target: 201802\n",
      "   ✅ Series válidas: 417638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_030110'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       4.47 GB / 15.69 GB (28.5%)\n",
      "Disk Space Avail:   138.89 GB / 459.95 GB (30.2%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 4794168 rows, 417638 time series. Median time series length is 12 (min=7, max=12). \n",
      "\tRemoving 20684 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 4638917 rows, 396954 time series. Median time series length is 12 (min=9, max=12). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 00:02:03\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.9s of the 3547.1s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.7s of the 3272.0s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8316       = Validation score (-WQL)\n",
      "\t15.82   s     = Training runtime\n",
      "\t7.26    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 295.3s of the 3248.5s of remaining time.\n",
      "\t-0.6160       = Validation score (-WQL)\n",
      "\t43.13   s     = Training runtime\n",
      "\t20.67   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 318.4s of the 3184.3s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 318.3s of the 2864.5s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 318.1s of the 2544.4s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 11384 time series (2.9%). Fallback model SeasonalNaive was used for these time series.\n",
      "\t-0.6813       = Validation score (-WQL)\n",
      "\t159.81  s     = Training runtime\n",
      "\t146.43  s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 319.7s of the 2238.0s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 372.8s of the 2237.1s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 447.0s of the 2235.1s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 558.5s of the 2233.8s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 816.4s of the 2232.9s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1632.0s of the 2232.0s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'AutoETS': 0.17, 'DirectTabular': 0.83}\n",
      "\t-0.6120       = Validation score (-WQL)\n",
      "\t131.86  s     = Training runtime\n",
      "\t167.10  s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'AutoETS', 'WeightedEnsemble']\n",
      "Total runtime: 1453.19 s\n",
      "Best model: WeightedEnsemble\n",
      "Best model score: -0.6120\n",
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1737.7 segundos.\n",
      "\n",
      "🔷 Iteración 6/27\n",
      "   📅 Cutoff: 201801, Target: 201803\n",
      "   ✅ Series válidas: 430282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_033007'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       4.39 GB / 15.69 GB (28.0%)\n",
      "Disk Space Avail:   138.02 GB / 459.95 GB (30.0%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 5282316 rows, 430282 time series. Median time series length is 13 (min=7, max=13). \n",
      "\tRemoving 26931 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 5082538 rows, 403351 time series. Median time series length is 13 (min=9, max=13). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 00:30:53\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 273.4s of the 3554.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 273.2s of the 3279.0s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8883       = Validation score (-WQL)\n",
      "\t18.64   s     = Training runtime\n",
      "\t8.60    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 295.6s of the 3251.3s of remaining time.\n",
      "\t-0.6311       = Validation score (-WQL)\n",
      "\t91.56   s     = Training runtime\n",
      "\t26.05   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 313.3s of the 3133.3s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 313.2s of the 2818.4s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 312.9s of the 2503.6s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 8228 time series (2.0%). Fallback model SeasonalNaive was used for these time series.\n",
      "\t-0.7348       = Validation score (-WQL)\n",
      "\t159.47  s     = Training runtime\n",
      "\t148.74  s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 313.6s of the 2195.1s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 365.7s of the 2194.1s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 438.4s of the 2192.0s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 547.6s of the 2190.6s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 794.8s of the 2189.6s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1588.6s of the 2188.6s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'AutoETS': 0.06, 'DirectTabular': 0.94}\n",
      "\t-0.6304       = Validation score (-WQL)\n",
      "\t138.05  s     = Training runtime\n",
      "\t174.79  s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'AutoETS', 'WeightedEnsemble']\n",
      "Total runtime: 1510.04 s\n",
      "Best model: WeightedEnsemble\n",
      "Best model score: -0.6304\n",
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1793.5 segundos.\n",
      "\n",
      "🔷 Iteración 7/27\n",
      "   📅 Cutoff: 201802, Target: 201804\n",
      "   ✅ Series válidas: 436832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_040001'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.05 GB / 15.69 GB (32.2%)\n",
      "Disk Space Avail:   137.10 GB / 459.95 GB (29.8%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 5735290 rows, 436832 time series. Median time series length is 14 (min=7, max=14). \n",
      "\tRemoving 26306 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 5534428 rows, 410526 time series. Median time series length is 14 (min=9, max=14). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 01:00:52\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 273.0s of the 3549.1s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.8s of the 3274.1s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.9354       = Validation score (-WQL)\n",
      "\t18.86   s     = Training runtime\n",
      "\t8.70    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 295.1s of the 3246.1s of remaining time.\n",
      "\t-0.6288       = Validation score (-WQL)\n",
      "\t66.19   s     = Training runtime\n",
      "\t17.78   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 316.2s of the 3161.7s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 315.9s of the 2843.5s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 315.7s of the 2525.7s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 8316 time series (2.0%). Fallback model SeasonalNaive was used for these time series.\n",
      "\t-0.7668       = Validation score (-WQL)\n",
      "\t163.06  s     = Training runtime\n",
      "\t147.59  s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 316.4s of the 2214.7s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 369.0s of the 2213.7s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 442.3s of the 2211.6s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 552.5s of the 2210.0s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 804.4s of the 2208.9s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1607.8s of the 2207.8s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6288       = Validation score (-WQL)\n",
      "\t137.50  s     = Training runtime\n",
      "\t17.78   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'AutoETS', 'WeightedEnsemble']\n",
      "Total runtime: 1485.86 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6288\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1607.7 segundos.\n",
      "\n",
      "🔷 Iteración 8/27\n",
      "   📅 Cutoff: 201803, Target: 201805\n",
      "   ✅ Series válidas: 443659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_042649'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.29 GB / 15.69 GB (33.7%)\n",
      "Disk Space Avail:   136.29 GB / 459.95 GB (29.6%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 6193627 rows, 443659 time series. Median time series length is 15 (min=7, max=15). \n",
      "\tRemoving 20505 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 6039480 rows, 423154 time series. Median time series length is 15 (min=9, max=15). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 01:27:39\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 273.1s of the 3549.8s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.9s of the 3274.9s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8797       = Validation score (-WQL)\n",
      "\t19.96   s     = Training runtime\n",
      "\t8.35    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 295.1s of the 3246.1s of remaining time.\n",
      "\t-0.6247       = Validation score (-WQL)\n",
      "\t69.36   s     = Training runtime\n",
      "\t19.12   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 315.7s of the 3157.1s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 315.5s of the 2839.7s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 315.3s of the 2522.4s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 11950 time series (2.8%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 314.9s of the 2204.6s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 367.2s of the 2203.5s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 440.2s of the 2201.2s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 549.9s of the 2199.5s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 799.1s of the 2198.2s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1597.0s of the 2197.0s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6247       = Validation score (-WQL)\n",
      "\t97.13   s     = Training runtime\n",
      "\t19.12   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1456.91 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6247\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1582.8 segundos.\n",
      "\n",
      "🔷 Iteración 9/27\n",
      "   📅 Cutoff: 201804, Target: 201806\n",
      "   ✅ Series válidas: 457309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_045313'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.86 GB / 15.69 GB (37.4%)\n",
      "Disk Space Avail:   135.59 GB / 459.95 GB (29.5%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 6701827 rows, 457309 time series. Median time series length is 16 (min=7, max=16). \n",
      "\tRemoving 27680 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 6497139 rows, 429629 time series. Median time series length is 16 (min=9, max=16). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 01:54:06\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.8s of the 3546.8s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.7s of the 3271.9s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8680       = Validation score (-WQL)\n",
      "\t21.00   s     = Training runtime\n",
      "\t8.98    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 294.7s of the 3241.5s of remaining time.\n",
      "\t-0.6275       = Validation score (-WQL)\n",
      "\t44.95   s     = Training runtime\n",
      "\t16.77   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 317.9s of the 3179.0s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 317.7s of the 2859.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 317.4s of the 2539.4s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 11991 time series (2.8%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 317.1s of the 2219.6s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 369.7s of the 2218.0s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 443.1s of the 2215.3s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 553.3s of the 2213.4s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 806.1s of the 2212.2s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1610.9s of the 2210.9s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6275       = Validation score (-WQL)\n",
      "\t103.88  s     = Training runtime\n",
      "\t16.77   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1447.15 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6275\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1580.1 segundos.\n",
      "\n",
      "🔷 Iteración 10/27\n",
      "   📅 Cutoff: 201805, Target: 201807\n",
      "   ✅ Series válidas: 470284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_051932'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       6.33 GB / 15.69 GB (40.4%)\n",
      "Disk Space Avail:   134.86 GB / 459.95 GB (29.3%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 7215277 rows, 470284 time series. Median time series length is 17 (min=7, max=17). \n",
      "\tRemoving 33888 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 6960775 rows, 436396 time series. Median time series length is 17 (min=9, max=17). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 02:20:27\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.7s of the 3545.6s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.6s of the 3270.7s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.7734       = Validation score (-WQL)\n",
      "\t22.82   s     = Training runtime\n",
      "\t10.73   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 294.2s of the 3236.7s of remaining time.\n",
      "\t-0.6188       = Validation score (-WQL)\n",
      "\t57.44   s     = Training runtime\n",
      "\t27.05   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 315.2s of the 3151.7s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 314.9s of the 2834.5s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 314.6s of the 2516.7s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 8835 time series (2.0%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 314.2s of the 2199.2s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 366.3s of the 2198.0s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 439.0s of the 2195.2s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 547.8s of the 2191.3s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 795.0s of the 2189.9s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1588.5s of the 2188.5s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6188       = Validation score (-WQL)\n",
      "\t101.83  s     = Training runtime\n",
      "\t27.05   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1467.17 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6188\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1605.8 segundos.\n",
      "\n",
      "🔷 Iteración 11/27\n",
      "   📅 Cutoff: 201806, Target: 201808\n",
      "   ✅ Series válidas: 471096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_054618'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.25 GB / 15.69 GB (33.5%)\n",
      "Disk Space Avail:   134.10 GB / 459.95 GB (29.2%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 7649697 rows, 471096 time series. Median time series length is 18 (min=7, max=18). \n",
      "\tRemoving 22100 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 7477375 rows, 448996 time series. Median time series length is 18 (min=9, max=18). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 02:47:12\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.8s of the 3546.3s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.6s of the 3271.3s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8060       = Validation score (-WQL)\n",
      "\t22.31   s     = Training runtime\n",
      "\t10.08   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 294.4s of the 3238.3s of remaining time.\n",
      "\t-0.6167       = Validation score (-WQL)\n",
      "\t75.45   s     = Training runtime\n",
      "\t33.80   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 312.9s of the 3128.5s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 312.6s of the 2813.4s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 312.3s of the 2498.7s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 11200 time series (2.5%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 311.7s of the 2181.8s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 363.4s of the 2180.5s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 435.5s of the 2177.6s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 543.3s of the 2173.1s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 785.8s of the 2171.6s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1570.2s of the 2170.2s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6167       = Validation score (-WQL)\n",
      "\t102.60  s     = Training runtime\n",
      "\t33.80   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1487.17 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6167\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1633.6 segundos.\n",
      "\n",
      "🔷 Iteración 12/27\n",
      "   📅 Cutoff: 201807, Target: 201809\n",
      "   ✅ Series válidas: 473741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_061332'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       6.17 GB / 15.69 GB (39.3%)\n",
      "Disk Space Avail:   133.29 GB / 459.95 GB (29.0%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 8094057 rows, 473741 time series. Median time series length is 19 (min=7, max=19). \n",
      "\tRemoving 11830 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 8005735 rows, 461911 time series. Median time series length is 19 (min=9, max=19). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 03:14:28\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.7s of the 3544.6s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.4s of the 3269.4s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8566       = Validation score (-WQL)\n",
      "\t25.30   s     = Training runtime\n",
      "\t10.85   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 293.9s of the 3232.7s of remaining time.\n",
      "\t-0.6217       = Validation score (-WQL)\n",
      "\t78.82   s     = Training runtime\n",
      "\t25.95   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 312.7s of the 3127.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 312.5s of the 2812.6s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 312.2s of the 2497.6s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 15360 time series (3.3%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 311.8s of the 2182.8s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 363.5s of the 2181.3s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 435.6s of the 2178.2s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 543.2s of the 2172.7s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 785.6s of the 2171.2s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1569.7s of the 2169.7s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6217       = Validation score (-WQL)\n",
      "\t108.57  s     = Training runtime\n",
      "\t25.95   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1492.59 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6217\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1629.2 segundos.\n",
      "\n",
      "🔷 Iteración 13/27\n",
      "   📅 Cutoff: 201808, Target: 201810\n",
      "   ✅ Series válidas: 476668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_064041'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       6.13 GB / 15.69 GB (39.1%)\n",
      "Disk Space Avail:   132.45 GB / 459.95 GB (28.8%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 8541449 rows, 476668 time series. Median time series length is 20 (min=7, max=20). \n",
      "\tRemoving 13955 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 8436409 rows, 462713 time series. Median time series length is 20 (min=9, max=20). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 03:41:35\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.8s of the 3546.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.6s of the 3271.2s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8173       = Validation score (-WQL)\n",
      "\t27.10   s     = Training runtime\n",
      "\t11.33   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 293.8s of the 3232.3s of remaining time.\n",
      "\t-0.6134       = Validation score (-WQL)\n",
      "\t121.95  s     = Training runtime\n",
      "\t61.56   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 304.8s of the 3048.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 304.5s of the 2740.7s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 304.2s of the 2433.8s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 10036 time series (2.2%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 303.8s of the 2126.9s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 354.1s of the 2124.8s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 424.2s of the 2120.9s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 528.7s of the 2115.0s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 756.6s of the 2113.2s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1511.6s of the 2111.6s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6134       = Validation score (-WQL)\n",
      "\t107.80  s     = Training runtime\n",
      "\t61.56   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1551.75 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6134\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1723.0 segundos.\n",
      "\n",
      "🔷 Iteración 14/27\n",
      "   📅 Cutoff: 201809, Target: 201811\n",
      "   ✅ Series válidas: 483070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_070924'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.56 GB / 15.69 GB (35.4%)\n",
      "Disk Space Avail:   131.58 GB / 459.95 GB (28.6%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 9013973 rows, 483070 time series. Median time series length is 21 (min=7, max=21). \n",
      "\tRemoving 17712 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 8882356 rows, 465358 time series. Median time series length is 21 (min=9, max=21). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 04:10:22\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.6s of the 3543.3s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.4s of the 3268.4s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8709       = Validation score (-WQL)\n",
      "\t32.19   s     = Training runtime\n",
      "\t17.52   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 292.6s of the 3218.1s of remaining time.\n",
      "\t-0.6205       = Validation score (-WQL)\n",
      "\t103.27  s     = Training runtime\n",
      "\t25.31   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 308.9s of the 3088.9s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 308.6s of the 2777.7s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 308.3s of the 2466.7s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 3511 time series (0.8%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 307.9s of the 2155.6s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 359.0s of the 2153.9s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 430.1s of the 2150.6s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 537.0s of the 2148.0s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 773.2s of the 2146.3s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1544.6s of the 2144.6s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6205       = Validation score (-WQL)\n",
      "\t108.00  s     = Training runtime\n",
      "\t25.31   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1516.23 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6205\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1657.3 segundos.\n",
      "\n",
      "🔷 Iteración 15/27\n",
      "   📅 Cutoff: 201810, Target: 201812\n",
      "   ✅ Series válidas: 491347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_073702'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       6.19 GB / 15.69 GB (39.5%)\n",
      "Disk Space Avail:   130.69 GB / 459.95 GB (28.4%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 9505492 rows, 491347 time series. Median time series length is 22 (min=7, max=22). \n",
      "\tRemoving 23067 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 9332912 rows, 468280 time series. Median time series length is 22 (min=9, max=22). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 04:37:57\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.8s of the 3545.9s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.5s of the 3269.7s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8258       = Validation score (-WQL)\n",
      "\t32.15   s     = Training runtime\n",
      "\t14.21   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 293.0s of the 3222.8s of remaining time.\n",
      "\t-0.6059       = Validation score (-WQL)\n",
      "\t55.67   s     = Training runtime\n",
      "\t20.95   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 314.5s of the 3145.4s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 314.3s of the 2828.3s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 313.9s of the 2511.0s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 4485 time series (1.0%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 313.4s of the 2193.9s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 365.4s of the 2192.2s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 437.8s of the 2188.8s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 546.5s of the 2186.0s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 792.1s of the 2184.3s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1582.4s of the 2182.4s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6059       = Validation score (-WQL)\n",
      "\t109.75  s     = Training runtime\n",
      "\t20.95   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1483.51 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6059\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1623.8 segundos.\n",
      "\n",
      "🔷 Iteración 16/27\n",
      "   📅 Cutoff: 201811, Target: 201901\n",
      "   ✅ Series válidas: 501923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_080406'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.91 GB / 15.69 GB (37.7%)\n",
      "Disk Space Avail:   129.78 GB / 459.95 GB (28.2%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 10018716 rows, 501923 time series. Median time series length is 23 (min=7, max=23). \n",
      "\tRemoving 27253 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 9814952 rows, 474670 time series. Median time series length is 23 (min=9, max=23). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 05:05:10\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.1s of the 3537.0s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 271.9s of the 3262.3s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8311       = Validation score (-WQL)\n",
      "\t39.86   s     = Training runtime\n",
      "\t13.65   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 291.7s of the 3208.3s of remaining time.\n",
      "\t-0.6106       = Validation score (-WQL)\n",
      "\t51.95   s     = Training runtime\n",
      "\t21.14   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 313.5s of the 3134.5s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 313.2s of the 2818.8s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 312.8s of the 2502.2s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 5788 time series (1.2%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 312.4s of the 2186.6s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 364.2s of the 2184.9s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 436.2s of the 2181.2s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 544.5s of the 2177.8s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 788.0s of the 2176.0s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1574.1s of the 2174.1s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6106       = Validation score (-WQL)\n",
      "\t112.64  s     = Training runtime\n",
      "\t21.14   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1486.19 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6106\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1636.2 segundos.\n",
      "\n",
      "🔷 Iteración 17/27\n",
      "   📅 Cutoff: 201812, Target: 201902\n",
      "   ✅ Series válidas: 506616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_083122'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       6.44 GB / 15.69 GB (41.0%)\n",
      "Disk Space Avail:   128.85 GB / 459.95 GB (28.0%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 10498107 rows, 506616 time series. Median time series length is 24 (min=7, max=24). \n",
      "\tRemoving 23680 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 10317074 rows, 482936 time series. Median time series length is 24 (min=9, max=24). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 05:32:23\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.3s of the 3540.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.1s of the 3265.3s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.9143       = Validation score (-WQL)\n",
      "\t33.55   s     = Training runtime\n",
      "\t12.20   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 292.6s of the 3219.0s of remaining time.\n",
      "\t-0.6255       = Validation score (-WQL)\n",
      "\t62.12   s     = Training runtime\n",
      "\t20.75   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 313.5s of the 3135.5s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 313.3s of the 2819.6s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 313.0s of the 2503.6s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 8632 time series (1.8%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 312.5s of the 2187.6s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 364.3s of the 2185.7s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 436.3s of the 2181.7s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 544.7s of the 2178.7s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 788.4s of the 2176.8s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1574.8s of the 2174.8s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6255       = Validation score (-WQL)\n",
      "\t112.01  s     = Training runtime\n",
      "\t20.75   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1488.65 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6255\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1641.7 segundos.\n",
      "\n",
      "🔷 Iteración 18/27\n",
      "   📅 Cutoff: 201901, Target: 201903\n",
      "   ✅ Series válidas: 511284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_085844'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.11 GB / 15.69 GB (32.6%)\n",
      "Disk Space Avail:   127.89 GB / 459.95 GB (27.8%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 10980387 rows, 511284 time series. Median time series length is 25 (min=7, max=25). \n",
      "\tRemoving 17817 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 10846248 rows, 493467 time series. Median time series length is 25 (min=9, max=25). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 05:59:44\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.3s of the 3540.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 272.1s of the 3265.1s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.9300       = Validation score (-WQL)\n",
      "\t31.96   s     = Training runtime\n",
      "\t13.12   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 292.7s of the 3219.5s of remaining time.\n",
      "\t-0.6249       = Validation score (-WQL)\n",
      "\t60.69   s     = Training runtime\n",
      "\t22.56   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 313.6s of the 3135.6s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 313.3s of the 2819.5s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 312.9s of the 2503.0s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 11006 time series (2.2%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 312.5s of the 2187.2s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 364.2s of the 2185.1s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 436.2s of the 2181.0s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 544.4s of the 2177.8s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 787.8s of the 2175.5s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1573.4s of the 2173.4s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6249       = Validation score (-WQL)\n",
      "\t114.10  s     = Training runtime\n",
      "\t22.56   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1492.49 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6249\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1642.8 segundos.\n",
      "\n",
      "🔷 Iteración 19/27\n",
      "   📅 Cutoff: 201902, Target: 201904\n",
      "   ✅ Series válidas: 517065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_092607'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.56 GB / 15.69 GB (35.5%)\n",
      "Disk Space Avail:   126.90 GB / 459.95 GB (27.6%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 11473488 rows, 517065 time series. Median time series length is 26 (min=7, max=26). \n",
      "\tRemoving 18935 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 11331527 rows, 498130 time series. Median time series length is 26 (min=9, max=26). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 06:27:10\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 272.1s of the 3537.8s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 271.9s of the 3262.8s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.9637       = Validation score (-WQL)\n",
      "\t35.14   s     = Training runtime\n",
      "\t13.99   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 292.1s of the 3213.2s of remaining time.\n",
      "\t-0.6307       = Validation score (-WQL)\n",
      "\t55.53   s     = Training runtime\n",
      "\t23.05   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 313.4s of the 3133.9s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 313.1s of the 2818.0s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 312.8s of the 2502.1s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 8036 time series (1.6%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 312.4s of the 2186.6s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 364.1s of the 2184.6s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 436.1s of the 2180.6s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 544.3s of the 2177.2s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 787.5s of the 2175.1s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1572.9s of the 2172.9s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6307       = Validation score (-WQL)\n",
      "\t115.66  s     = Training runtime\n",
      "\t23.05   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1493.21 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6307\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1646.7 segundos.\n",
      "\n",
      "🔷 Iteración 20/27\n",
      "   📅 Cutoff: 201903, Target: 201905\n",
      "   ✅ Series válidas: 535833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_095334'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       6.15 GB / 15.69 GB (39.2%)\n",
      "Disk Space Avail:   125.88 GB / 459.95 GB (27.4%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 12055593 rows, 535833 time series. Median time series length is 27 (min=7, max=27). \n",
      "\tRemoving 33086 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 11813453 rows, 502747 time series. Median time series length is 27 (min=9, max=27). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 06:54:41\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 271.8s of the 3533.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 271.5s of the 3258.4s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.9629       = Validation score (-WQL)\n",
      "\t34.75   s     = Training runtime\n",
      "\t14.47   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 291.7s of the 3208.6s of remaining time.\n",
      "\t-0.6305       = Validation score (-WQL)\n",
      "\t66.96   s     = Training runtime\n",
      "\t23.68   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 311.7s of the 3117.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 311.4s of the 2802.8s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 311.1s of the 2488.5s of remaining time.\n",
      "\tWarning: AutoETS\\W0 failed for 5808 time series (1.2%). Fallback model SeasonalNaive was used for these time series.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 310.7s of the 2174.6s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 362.1s of the 2172.5s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 433.6s of the 2168.2s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 541.2s of the 2164.7s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 781.3s of the 2162.5s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1560.3s of the 2160.3s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6305       = Validation score (-WQL)\n",
      "\t119.13  s     = Training runtime\n",
      "\t23.68   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1504.90 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6305\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1668.5 segundos.\n",
      "\n",
      "🔷 Iteración 21/27\n",
      "   📅 Cutoff: 201904, Target: 201906\n",
      "   ✅ Series válidas: 549964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_102123'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       6.34 GB / 15.69 GB (40.4%)\n",
      "Disk Space Avail:   124.83 GB / 459.95 GB (27.1%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 12616265 rows, 549964 time series. Median time series length is 28 (min=7, max=28). \n",
      "\tRemoving 41520 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 12302072 rows, 508444 time series. Median time series length is 28 (min=9, max=28). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 07:22:30\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 271.8s of the 3533.4s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 271.5s of the 3258.6s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8892       = Validation score (-WQL)\n",
      "\t35.49   s     = Training runtime\n",
      "\t14.19   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 291.7s of the 3208.3s of remaining time.\n",
      "\t-0.6208       = Validation score (-WQL)\n",
      "\t86.09   s     = Training runtime\n",
      "\t33.99   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 308.7s of the 3087.4s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 308.4s of the 2775.9s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 310.2s of the 2481.6s of remaining time.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 309.7s of the 2168.1s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 361.0s of the 2165.8s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 432.2s of the 2161.2s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 539.4s of the 2157.5s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 777.6s of the 2155.1s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1552.8s of the 2152.8s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6208       = Validation score (-WQL)\n",
      "\t119.77  s     = Training runtime\n",
      "\t33.99   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1513.22 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6208\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1696.7 segundos.\n",
      "\n",
      "🔷 Iteración 22/27\n",
      "   📅 Cutoff: 201905, Target: 201907\n",
      "   ✅ Series válidas: 558318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_104939'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       6.06 GB / 15.69 GB (38.6%)\n",
      "Disk Space Avail:   123.73 GB / 459.95 GB (26.9%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 13140657 rows, 558318 time series. Median time series length is 28 (min=7, max=29). \n",
      "\tRemoving 31234 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 12902993 rows, 527084 time series. Median time series length is 29 (min=9, max=29). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 07:50:45\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 271.9s of the 3534.8s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 271.7s of the 3259.9s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8919       = Validation score (-WQL)\n",
      "\t36.81   s     = Training runtime\n",
      "\t15.12   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 291.6s of the 3207.4s of remaining time.\n",
      "\t-0.6298       = Validation score (-WQL)\n",
      "\t82.08   s     = Training runtime\n",
      "\t36.20   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 308.8s of the 3088.2s of remaining time.\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -11.4s)\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 307.5s of the 2767.9s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 308.8s of the 2470.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 308.3s of the 2158.0s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 359.3s of the 2155.7s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 430.1s of the 2150.5s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 536.4s of the 2145.7s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 771.7s of the 2143.3s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1540.7s of the 2140.7s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6298       = Validation score (-WQL)\n",
      "\t125.29  s     = Training runtime\n",
      "\t36.20   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1533.39 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6298\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1713.7 segundos.\n",
      "\n",
      "🔷 Iteración 23/27\n",
      "   📅 Cutoff: 201906, Target: 201908\n",
      "   ✅ Series válidas: 559984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_111814'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       6.91 GB / 15.69 GB (44.0%)\n",
      "Disk Space Avail:   122.59 GB / 459.95 GB (26.7%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 13615467 rows, 559984 time series. Median time series length is 29 (min=7, max=30). \n",
      "\tRemoving 18807 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 13470571 rows, 541177 time series. Median time series length is 29 (min=9, max=30). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 08:19:24\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 271.6s of the 3531.0s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 271.2s of the 3254.9s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8933       = Validation score (-WQL)\n",
      "\t38.43   s     = Training runtime\n",
      "\t16.82   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 290.8s of the 3199.1s of remaining time.\n",
      "\t-0.6375       = Validation score (-WQL)\n",
      "\t133.02  s     = Training runtime\n",
      "\t48.07   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 301.7s of the 3017.3s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 301.2s of the 2711.2s of remaining time.\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -22.7s)\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 298.4s of the 2387.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 297.8s of the 2084.4s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 347.0s of the 2081.8s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 415.2s of the 2075.8s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 517.9s of the 2071.4s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 734.4s of the 2068.9s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1466.3s of the 2066.3s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6375       = Validation score (-WQL)\n",
      "\t132.19  s     = Training runtime\n",
      "\t48.07   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1611.88 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6375\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1802.9 segundos.\n",
      "\n",
      "🔷 Iteración 24/27\n",
      "   📅 Cutoff: 201907, Target: 201909\n",
      "   ✅ Series válidas: 560807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_114818'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.60 GB / 15.69 GB (35.7%)\n",
      "Disk Space Avail:   121.41 GB / 459.95 GB (26.4%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 14080462 rows, 560807 time series. Median time series length is 30 (min=7, max=31). \n",
      "\tRemoving 11336 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 13994511 rows, 549471 time series. Median time series length is 30 (min=9, max=31). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 08:49:30\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 271.4s of the 3528.6s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 271.1s of the 3253.8s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.8919       = Validation score (-WQL)\n",
      "\t40.38   s     = Training runtime\n",
      "\t16.63   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 290.6s of the 3196.1s of remaining time.\n",
      "\t-0.6345       = Validation score (-WQL)\n",
      "\t99.67   s     = Training runtime\n",
      "\t29.37   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 306.6s of the 3066.4s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 306.2s of the 2755.8s of remaining time.\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -21.3s)\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 303.5s of the 2428.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 303.0s of the 2121.1s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 353.1s of the 2118.4s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 422.6s of the 2113.2s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 527.3s of the 2109.0s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 753.1s of the 2106.2s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1503.6s of the 2103.6s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6345       = Validation score (-WQL)\n",
      "\t133.19  s     = Training runtime\n",
      "\t29.37   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1573.21 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6345\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1754.9 segundos.\n",
      "\n",
      "🔷 Iteración 25/27\n",
      "   📅 Cutoff: 201908, Target: 201910\n",
      "   ✅ Series válidas: 564665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_121734'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.87 GB / 15.69 GB (37.4%)\n",
      "Disk Space Avail:   120.20 GB / 459.95 GB (26.1%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 14558581 rows, 564665 time series. Median time series length is 31 (min=7, max=32). \n",
      "\tRemoving 13580 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 14457729 rows, 551085 time series. Median time series length is 31 (min=9, max=32). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 09:18:51\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 271.1s of the 3524.0s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 270.7s of the 3248.6s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.9546       = Validation score (-WQL)\n",
      "\t41.83   s     = Training runtime\n",
      "\t17.01   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 289.9s of the 3189.2s of remaining time.\n",
      "\t-0.6527       = Validation score (-WQL)\n",
      "\t119.09  s     = Training runtime\n",
      "\t33.23   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 303.6s of the 3036.0s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 303.1s of the 2728.3s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 302.6s of the 2421.1s of remaining time.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 302.1s of the 2114.6s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 352.0s of the 2112.0s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 421.2s of the 2106.2s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 525.2s of the 2100.9s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 749.0s of the 2098.0s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1495.0s of the 2095.0s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6527       = Validation score (-WQL)\n",
      "\t138.14  s     = Training runtime\n",
      "\t33.23   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1583.87 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6527\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1783.5 segundos.\n",
      "\n",
      "🔷 Iteración 26/27\n",
      "   📅 Cutoff: 201909, Target: 201911\n",
      "   ✅ Series válidas: 578531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_124716'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.71 GB / 15.69 GB (36.4%)\n",
      "Disk Space Avail:   118.98 GB / 459.95 GB (25.9%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 15103882 rows, 578531 time series. Median time series length is 32 (min=7, max=33). \n",
      "\tRemoving 26651 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 14908494 rows, 551880 time series. Median time series length is 32 (min=9, max=33). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 09:48:29\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 271.3s of the 3527.5s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 271.0s of the 3252.2s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.9174       = Validation score (-WQL)\n",
      "\t51.43   s     = Training runtime\n",
      "\t22.63   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 288.9s of the 3177.5s of remaining time.\n",
      "\t-0.6511       = Validation score (-WQL)\n",
      "\t102.90  s     = Training runtime\n",
      "\t31.22   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 304.3s of the 3042.5s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 303.8s of the 2734.0s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 303.3s of the 2426.0s of remaining time.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 302.7s of the 2119.2s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 352.7s of the 2116.1s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 422.1s of the 2110.4s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 526.3s of the 2105.0s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 750.7s of the 2101.4s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1498.4s of the 2098.4s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6511       = Validation score (-WQL)\n",
      "\t135.08  s     = Training runtime\n",
      "\t31.22   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1580.49 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6511\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1767.0 segundos.\n",
      "\n",
      "🔷 Iteración 27/27\n",
      "   📅 Cutoff: 201910, Target: 201912\n",
      "   ✅ Series válidas: 600305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 3600s\n",
      "AutoGluon will save models to 'c:\\Maestria Ciencia de Datos\\Labo 3\\TP\\Dataset\\AutogluonModels\\ag-20250714_131647'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.22\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.76 GB / 15.69 GB (36.7%)\n",
      "Disk Space Avail:   117.66 GB / 459.95 GB (25.6%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'tn',\n",
      " 'time_limit': 3600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🚀 Entrenando predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided train_data has 15708766 rows, 600305 time series. Median time series length is 32 (min=7, max=34). \n",
      "\tRemoving 44625 short time series from train_data. Only series with length >= 9 will be used for training.\n",
      "\tAfter filtering, train_data has 15377596 rows, 555680 time series. Median time series length is 33 (min=9, max=34). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'tn'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['periodo', 'product_id', 'customer_id']\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-07-14 10:18:10\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 270.6s of the 3518.0s of remaining time.\n",
      "\tTime limit exceeded... Skipping SeasonalNaive.\n",
      "Training timeseries model RecursiveTabular. Training for up to 270.3s of the 3243.8s of remaining time.\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\tTime series in the dataset are too short for chosen differences [12]. Setting differences to [1].\n",
      "\t-0.9251       = Validation score (-WQL)\n",
      "\t48.96   s     = Training runtime\n",
      "\t21.67   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 288.4s of the 3172.4s of remaining time.\n",
      "\t-0.6490       = Validation score (-WQL)\n",
      "\t71.23   s     = Training runtime\n",
      "\t30.66   s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 307.0s of the 3069.6s of remaining time.\n",
      "\tTime limit exceeded... Skipping NPTS.\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 306.5s of the 2758.5s of remaining time.\n",
      "\tTime limit exceeded... Skipping DynamicOptimizedTheta.\n",
      "Training timeseries model AutoETS. Training for up to 305.9s of the 2447.6s of remaining time.\n",
      "\tTime limit exceeded... Skipping AutoETS.\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 305.4s of the 2137.5s of remaining time.\n",
      "\tWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 355.8s of the 2134.8s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\n",
      "\tFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 425.9s of the 2129.3s of remaining time.\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model DeepAR. Training for up to 531.1s of the 2124.2s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model PatchTST. Training for up to 760.6s of the 2121.3s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Training timeseries model TiDE. Training for up to 1518.2s of the 2118.2s of remaining time.\n",
      "\tWarning: Exception caused TiDE to fail during training... Skipping this model.\n",
      "\tpartially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.6490       = Validation score (-WQL)\n",
      "\t134.42  s     = Training runtime\n",
      "\t30.66   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'WeightedEnsemble']\n",
      "Total runtime: 1550.84 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.6490\n",
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ⏱ Iteración completada en 1754.0 segundos.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "# Asegúrate de tener las importaciones de la librería que usas (ej: autogluon)\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "\n",
    "# --- Carga de datos (asumimos que 'df' ya está cargado) ---\n",
    "# df = pd.read_csv('tus_datos.csv') \n",
    "# df['periodo'] = df['periodo'].astype(str) # Asegurar que periodo es string\n",
    "\n",
    "# --- INICIO DEL SCRIPT ---\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "\n",
    "# Combina las dos columnas para identificar la serie\n",
    "df['item_id'] = df['product_id'].astype(str) + \"_\" + df['customer_id'].astype(str)\n",
    "\n",
    "# Guarda los tipos de datos originales para usarlos después\n",
    "original_types = {'product_id': df['product_id'].dtype, 'customer_id': df['customer_id'].dtype}\n",
    "\n",
    "# 2. Lista de periodos ordenada\n",
    "periods = sorted(df['timestamp'].unique())\n",
    "\n",
    "# 4. Loop roll-forward\n",
    "results = []\n",
    "min_length = 7\n",
    "start_idx = min_length + 2\n",
    "\n",
    "for idx in range(start_idx, len(periods)):\n",
    "    start_time = time.time()\n",
    "    cutoff = periods[idx-2]\n",
    "    target_period = periods[idx]\n",
    "    cutoff_str = pd.to_datetime(cutoff).strftime('%Y%m')\n",
    "    target_str = pd.to_datetime(target_period).strftime('%Y%m')\n",
    "\n",
    "    print(f\"\\n🔷 Iteración {idx-start_idx+1}/{len(periods)-start_idx}\")\n",
    "    print(f\"   📅 Cutoff: {cutoff_str}, Target: {target_str}\")\n",
    "\n",
    "    df_train = df[df['timestamp'] <= cutoff]\n",
    "\n",
    "    counts = df_train.groupby('item_id').size()\n",
    "    valid_ids = counts[counts >= min_length].index.tolist()\n",
    "    if not valid_ids:\n",
    "        print(\"   ⚠ No hay series con longitud suficiente, se salta esta iteración.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"   ✅ Series válidas: {len(valid_ids)}\")\n",
    "    df_train = df_train[df_train['item_id'].isin(valid_ids)]\n",
    "    n_meses = df_train['timestamp'].nunique()\n",
    "\n",
    "    ts_train = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_train,\n",
    "        id_column='item_id',\n",
    "        timestamp_column='timestamp'\n",
    "    ).fill_missing_values() # SUGERENCIA: Considera fill_missing_values(method='ffill') si no quieres rellenar con 0\n",
    "\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        prediction_length=2,\n",
    "        target='tn', # Asegúrate que la columna 'tn' exista en ts_train\n",
    "        freq='MS'\n",
    "    )\n",
    "    print(\"   🚀 Entrenando predictor...\")\n",
    "    predictor.fit(ts_train, num_val_windows=2, time_limit=60*60)\n",
    "\n",
    "    forecast = predictor.predict(ts_train)\n",
    "\n",
    "    # CORRECCIÓN: Se elimina el mapeo redundante 'item_id': 'item_id'\n",
    "    df_pred = forecast['mean'].reset_index().rename(columns={'mean': 'tn_pred_auto'})\n",
    "\n",
    "    df_pred = df_pred[df_pred['timestamp'] == target_period]\n",
    "\n",
    "    # Separa nuevamente product_id y customer_id\n",
    "    df_pred[['product_id', 'customer_id']] = df_pred['item_id'].str.split('_', expand=True)\n",
    "\n",
    "    # CORRECCIÓN (BUENA PRÁCTICA): Se evita inplace=True\n",
    "    df_pred = df_pred.drop(columns=['item_id'])\n",
    "\n",
    "    # CORRECCIÓN (CRÍTICA): Convierte las columnas a su tipo de dato original\n",
    "    df_pred = df_pred.astype({'product_id': original_types['product_id'],\n",
    "                              'customer_id': original_types['customer_id']})\n",
    "\n",
    "    df_pred['n_meses_hist'] = n_meses\n",
    "    df_pred['periodo_pred'] = cutoff_str\n",
    "    \n",
    "    # La selección de columnas final es una excelente práctica\n",
    "    results.append(df_pred[['product_id', 'customer_id', 'periodo_pred', 'timestamp', 'tn_pred_auto', 'n_meses_hist']])\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"   ⏱ Iteración completada en {elapsed:.1f} segundos.\")\n",
    "\n",
    "# Al final, puedes concatenar todos los resultados en un único DataFrame\n",
    "final_results_df = pd.concat(results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad36f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_preds = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a77be9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>periodo_pred</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tn_pred_auto</th>\n",
       "      <th>n_meses_hist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>10001</td>\n",
       "      <td>201708</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>75.478121</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>10002</td>\n",
       "      <td>201708</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>20.881114</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20001</td>\n",
       "      <td>10003</td>\n",
       "      <td>201708</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>80.516141</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20001</td>\n",
       "      <td>10004</td>\n",
       "      <td>201708</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>147.217463</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20001</td>\n",
       "      <td>10005</td>\n",
       "      <td>201708</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>14.255742</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  customer_id periodo_pred  timestamp  tn_pred_auto  n_meses_hist\n",
       "0       20001        10001       201708 2017-10-01     75.478121             8\n",
       "1       20001        10002       201708 2017-10-01     20.881114             8\n",
       "2       20001        10003       201708 2017-10-01     80.516141             8\n",
       "3       20001        10004       201708 2017-10-01    147.217463             8\n",
       "4       20001        10005       201708 2017-10-01     14.255742             8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2084d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_preds['periodo'] = df_all_preds['timestamp'].dt.strftime('%Y%m').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47a0eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['customer_id']  = df['customer_id'].astype(np.int32)\n",
    "df['product_id']   = df['product_id'].astype(np.int32)\n",
    "df['periodo']      = df['periodo'].astype(np.int32)\n",
    "\n",
    "df_all_preds['customer_id'] = df_all_preds['customer_id'].astype(np.int32)\n",
    "df_all_preds['product_id']  = df_all_preds['product_id'].astype(np.int32)\n",
    "df_all_preds['periodo']     = df_all_preds['periodo'].astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d53638c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_all_preds['periodo_pred']\n",
    "del df_all_preds['timestamp'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a1dd784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>tn_pred_auto</th>\n",
       "      <th>n_meses_hist</th>\n",
       "      <th>periodo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>10001</td>\n",
       "      <td>75.478121</td>\n",
       "      <td>8</td>\n",
       "      <td>201710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>10002</td>\n",
       "      <td>20.881114</td>\n",
       "      <td>8</td>\n",
       "      <td>201710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20001</td>\n",
       "      <td>10003</td>\n",
       "      <td>80.516141</td>\n",
       "      <td>8</td>\n",
       "      <td>201710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20001</td>\n",
       "      <td>10004</td>\n",
       "      <td>147.217463</td>\n",
       "      <td>8</td>\n",
       "      <td>201710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20001</td>\n",
       "      <td>10005</td>\n",
       "      <td>14.255742</td>\n",
       "      <td>8</td>\n",
       "      <td>201710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  customer_id  tn_pred_auto  n_meses_hist  periodo\n",
       "0       20001        10001     75.478121             8   201710\n",
       "1       20001        10002     20.881114             8   201710\n",
       "2       20001        10003     80.516141             8   201710\n",
       "3       20001        10004    147.217463             8   201710\n",
       "4       20001        10005     14.255742             8   201710"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_preds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5847645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_preds.to_csv('predicciones_autogluon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cadbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"cliente_producto_base.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e13c96a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria inicial del DataFrame: 5635.93 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_24920\\3492438140.py:31: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(df[col]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria final del DataFrame: 5574.67 MB\n",
      "Memoria reducida en un 1.09%\n"
     ]
    }
   ],
   "source": [
    "df=reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c4293ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria inicial del DataFrame: 314.44 MB\n",
      "Uso de memoria final del DataFrame: 123.53 MB\n",
      "Memoria reducida en un 60.71%\n"
     ]
    }
   ],
   "source": [
    "df_all_preds=reduce_mem_usage(df_all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e873e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "   df_all_preds,\n",
    "    on=['customer_id','product_id','periodo'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa9876d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cliente_activo</th>\n",
       "      <th>producto_activo</th>\n",
       "      <th>inicio_vida_c</th>\n",
       "      <th>inicio_vida_p</th>\n",
       "      <th>...</th>\n",
       "      <th>cliente_total_delta_min_movil_6</th>\n",
       "      <th>cliente_total_delta_min_movil_12</th>\n",
       "      <th>cliente_total_delta_std_movil_3</th>\n",
       "      <th>cliente_total_delta_std_movil_</th>\n",
       "      <th>cliente_total_delta_std_movil_12</th>\n",
       "      <th>cliente_trend</th>\n",
       "      <th>cliente_season_yearly</th>\n",
       "      <th>clase</th>\n",
       "      <th>tn_pred_auto</th>\n",
       "      <th>n_meses_hist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201701</td>\n",
       "      <td>11</td>\n",
       "      <td>99.437500</td>\n",
       "      <td>99.437500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>201701</td>\n",
       "      <td>201701</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>20001</td>\n",
       "      <td>201701</td>\n",
       "      <td>17</td>\n",
       "      <td>38.687500</td>\n",
       "      <td>35.718750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>201701</td>\n",
       "      <td>201701</td>\n",
       "      <td>...</td>\n",
       "      <td>3144.0</td>\n",
       "      <td>3144.0</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>3142.0</td>\n",
       "      <td>2512.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>20001</td>\n",
       "      <td>201701</td>\n",
       "      <td>17</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>201701</td>\n",
       "      <td>201701</td>\n",
       "      <td>...</td>\n",
       "      <td>2424.0</td>\n",
       "      <td>2426.0</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>1468.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-124.4375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>20001</td>\n",
       "      <td>201701</td>\n",
       "      <td>9</td>\n",
       "      <td>184.750000</td>\n",
       "      <td>184.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>201701</td>\n",
       "      <td>201701</td>\n",
       "      <td>...</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>878.0</td>\n",
       "      <td>1127.0</td>\n",
       "      <td>1306.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-140.8750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>20001</td>\n",
       "      <td>201701</td>\n",
       "      <td>23</td>\n",
       "      <td>19.078125</td>\n",
       "      <td>19.078125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>201701</td>\n",
       "      <td>201701</td>\n",
       "      <td>...</td>\n",
       "      <td>1059.0</td>\n",
       "      <td>1059.0</td>\n",
       "      <td>1058.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>663.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  periodo  cust_request_qty  cust_request_tn  \\\n",
       "0        10001       20001   201701                11        99.437500   \n",
       "1        10002       20001   201701                17        38.687500   \n",
       "2        10003       20001   201701                17       143.500000   \n",
       "3        10004       20001   201701                 9       184.750000   \n",
       "4        10005       20001   201701                23        19.078125   \n",
       "\n",
       "           tn  cliente_activo  producto_activo  inicio_vida_c  inicio_vida_p  \\\n",
       "0   99.437500             1.0              1.0         201701         201701   \n",
       "1   35.718750             1.0              1.0         201701         201701   \n",
       "2  143.500000             1.0              1.0         201701         201701   \n",
       "3  184.750000             1.0              1.0         201701         201701   \n",
       "4   19.078125             1.0              1.0         201701         201701   \n",
       "\n",
       "   ...  cliente_total_delta_min_movil_6  cliente_total_delta_min_movil_12  \\\n",
       "0  ...                              NaN                               NaN   \n",
       "1  ...                           3144.0                            3144.0   \n",
       "2  ...                           2424.0                            2426.0   \n",
       "3  ...                           1738.0                            1738.0   \n",
       "4  ...                           1059.0                            1059.0   \n",
       "\n",
       "   cliente_total_delta_std_movil_3  cliente_total_delta_std_movil_  \\\n",
       "0                              NaN                             NaN   \n",
       "1                           3140.0                          3142.0   \n",
       "2                           1623.0                          1132.0   \n",
       "3                            878.0                          1127.0   \n",
       "4                           1058.0                          1056.0   \n",
       "\n",
       "  cliente_total_delta_std_movil_12 cliente_trend cliente_season_yearly  \\\n",
       "0                              NaN           NaN                   NaN   \n",
       "1                           2512.0           NaN                   NaN   \n",
       "2                           1468.0           NaN                   NaN   \n",
       "3                           1306.0           NaN                   NaN   \n",
       "4                            663.0           NaN                   NaN   \n",
       "\n",
       "      clase  tn_pred_auto  n_meses_hist  \n",
       "0   44.0625           NaN           NaN  \n",
       "1  149.0000           NaN           NaN  \n",
       "2 -124.4375           NaN           NaN  \n",
       "3 -140.8750           NaN           NaN  \n",
       "4   46.0625           NaN           NaN  \n",
       "\n",
       "[5 rows x 178 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7dfaeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria inicial del DataFrame: 6190.84 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diana\\AppData\\Local\\Temp\\ipykernel_24920\\3492438140.py:31: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(df[col]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria final del DataFrame: 5830.53 MB\n",
      "Memoria reducida en un 5.82%\n"
     ]
    }
   ],
   "source": [
    "df['tn_pred_auto_delta_a_tn']=df['tn_pred_auto']-df['tn'].fillna(0)\n",
    "df['ratio_tn_pred_auto_delta_a_tn']=percentage_safe(df['tn_pred_auto_delta_a_tn'],df['tn'])\n",
    "df['ratio_tn_pred_a_tn']=percentage_safe(df['tn_pred_auto'],df['tn'])\n",
    "\n",
    "df=reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6537b046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>product_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>tn</th>\n",
       "      <th>clase</th>\n",
       "      <th>tn_pred_auto</th>\n",
       "      <th>tn_pred_auto_delta_a_tn</th>\n",
       "      <th>ratio_tn_pred_auto_delta_a_tn</th>\n",
       "      <th>ratio_tn_pred_a_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>n_meses_hist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>10001</td>\n",
       "      <td>99.437500</td>\n",
       "      <td>44.062500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.437500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>10002</td>\n",
       "      <td>35.718750</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.718750</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>10003</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>-124.437500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>10004</td>\n",
       "      <td>184.750000</td>\n",
       "      <td>-140.875000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>184.750000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>10005</td>\n",
       "      <td>19.078125</td>\n",
       "      <td>46.062500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.078125</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>10097</td>\n",
       "      <td>0.615723</td>\n",
       "      <td>-0.246338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.615723</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>10098</td>\n",
       "      <td>3.939453</td>\n",
       "      <td>-3.570312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.939453</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>10099</td>\n",
       "      <td>0.369385</td>\n",
       "      <td>-0.369385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.369385</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>10101</td>\n",
       "      <td>0.369385</td>\n",
       "      <td>-0.369385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.369385</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>10102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    periodo  product_id  customer_id          tn       clase  tn_pred_auto  \\\n",
       "0    201701       20001        10001   99.437500   44.062500           NaN   \n",
       "1    201701       20001        10002   35.718750  149.000000           NaN   \n",
       "2    201701       20001        10003  143.500000 -124.437500           NaN   \n",
       "3    201701       20001        10004  184.750000 -140.875000           NaN   \n",
       "4    201701       20001        10005   19.078125   46.062500           NaN   \n",
       "..      ...         ...          ...         ...         ...           ...   \n",
       "95   201701       20001        10097    0.615723   -0.246338           NaN   \n",
       "96   201701       20001        10098    3.939453   -3.570312           NaN   \n",
       "97   201701       20001        10099    0.369385   -0.369385           NaN   \n",
       "98   201701       20001        10101    0.369385   -0.369385           NaN   \n",
       "99   201701       20001        10102    0.000000    0.134277           NaN   \n",
       "\n",
       "    tn_pred_auto_delta_a_tn  ratio_tn_pred_auto_delta_a_tn  \\\n",
       "0                       NaN                            NaN   \n",
       "1                       NaN                            NaN   \n",
       "2                       NaN                            NaN   \n",
       "3                       NaN                            NaN   \n",
       "4                       NaN                            NaN   \n",
       "..                      ...                            ...   \n",
       "95                      NaN                            NaN   \n",
       "96                      NaN                            NaN   \n",
       "97                      NaN                            NaN   \n",
       "98                      NaN                            NaN   \n",
       "99                      NaN                            NaN   \n",
       "\n",
       "    ratio_tn_pred_a_tn          tn  n_meses_hist  \n",
       "0                  NaN   99.437500           NaN  \n",
       "1                  NaN   35.718750           NaN  \n",
       "2                  NaN  143.500000           NaN  \n",
       "3                  NaN  184.750000           NaN  \n",
       "4                  NaN   19.078125           NaN  \n",
       "..                 ...         ...           ...  \n",
       "95                 NaN    0.615723           NaN  \n",
       "96                 NaN    3.939453           NaN  \n",
       "97                 NaN    0.369385           NaN  \n",
       "98                 NaN    0.369385           NaN  \n",
       "99                 NaN    0.000000           NaN  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['periodo','product_id','customer_id','tn','clase','tn_pred_auto','tn_pred_auto_delta_a_tn','ratio_tn_pred_auto_delta_a_tn','ratio_tn_pred_a_tn','tn','n_meses_hist']].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c10263f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "float16 indexes are not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtn_pred_auto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\neural_env\\lib\\site-packages\\pandas\\core\\frame.py:7509\u001b[0m, in \u001b[0;36mDataFrame.value_counts\u001b[1;34m(self, subset, normalize, sort, ascending, dropna)\u001b[0m\n\u001b[0;32m   7506\u001b[0m     subset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m   7508\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproportion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 7509\u001b[0m counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7510\u001b[0m counts\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   7512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\neural_env\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:705\u001b[0m, in \u001b[0;36mBaseGrouper.size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msize\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m    702\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;124;03m    Compute group sizes.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 705\u001b[0m     ids, _, ngroups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_info\u001b[49m\n\u001b[0;32m    706\u001b[0m     out: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngroups:\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\neural_env\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:745\u001b[0m, in \u001b[0;36mBaseGrouper.group_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgroup_info\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m--> 745\u001b[0m     comp_ids, obs_group_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_compressed_codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m     ngroups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(obs_group_ids)\n\u001b[0;32m    748\u001b[0m     comp_ids \u001b[38;5;241m=\u001b[39m ensure_platform_int(comp_ids)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\neural_env\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:769\u001b[0m, in \u001b[0;36mBaseGrouper._get_compressed_codes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;66;03m# FIXME: compress_group_index's second return value is int64, not intp\u001b[39;00m\n\u001b[0;32m    768\u001b[0m ping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupings[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ping\u001b[38;5;241m.\u001b[39mcodes, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[43mping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_group_index\u001b[49m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\neural_env\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:765\u001b[0m, in \u001b[0;36mGrouping._group_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m             new_codes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minsert(uniques\u001b[38;5;241m.\u001b[39mcodes, na_unique_idx, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    762\u001b[0m             uniques \u001b[38;5;241m=\u001b[39m Categorical\u001b[38;5;241m.\u001b[39mfrom_codes(\n\u001b[0;32m    763\u001b[0m                 new_codes, uniques\u001b[38;5;241m.\u001b[39mcategories, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    764\u001b[0m             )\n\u001b[1;32m--> 765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43muniques\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\neural_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:680\u001b[0m, in \u001b[0;36mIndex._with_infer\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_with_infer\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;124;03m    Constructor that uses the 1.0.x behavior inferring numeric dtypes\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03m    for ndarray[object] inputs.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 680\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_is_multi:\n\u001b[0;32m    683\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"maybe_convert_objects\" has incompatible type\u001b[39;00m\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected\u001b[39;00m\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;66;03m# \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m    686\u001b[0m         values \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result\u001b[38;5;241m.\u001b[39m_values)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\neural_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:576\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[0;32m    572\u001b[0m arr \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(arr)\n\u001b[0;32m    574\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype_to_subclass(arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 576\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m result \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39m_simple_new(arr, name, refs\u001b[38;5;241m=\u001b[39mrefs)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_pandas_object \u001b[38;5;129;01mand\u001b[39;00m data_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\neural_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:601\u001b[0m, in \u001b[0;36mIndex._ensure_array\u001b[1;34m(cls, data, dtype, copy)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex data must be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;66;03m# float16 not supported (no indexing engine)\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16 indexes are not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;66;03m# asarray_tuplesafe does not always copy underlying data,\u001b[39;00m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m#  so need to make sure that this happens\u001b[39;00m\n\u001b[0;32m    606\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: float16 indexes are not supported"
     ]
    }
   ],
   "source": [
    "df[['tn_pred_auto']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a278683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('0_b_cliente_producto_autogluon_completo.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1922bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = df[~df['periodo'].isin([201911, 201912])]\n",
    "df_train = df.query(\"periodo != 201911 and periodo != 201912\")\n",
    "\n",
    "df_train.to_parquet('0_b_cliente_producto_autogluon_train.parquet', index=False)\n",
    "print(f\"DataFrame de entrenamiento guardado en 'train.parquet' con {len(df_train)} filas.\")\n",
    "\n",
    "# --- 3. Preparar y guardar el DataFrame de predicción en Parquet ---\n",
    "# Seleccionamos los periodos 201911 y 201912 para el conjunto de predicción.\n",
    "# Eliminamos la columna 'clase' ya que no será necesaria para la predicción.\n",
    "# Finalmente, guardamos este DataFrame en un archivo Parquet.\n",
    "df_predecir = df[df['periodo'].isin([201911, 201912])].copy() # Usar .copy() para evitar SettingWithCopyWarning\n",
    "df_predecir.drop(columns=['clase'], inplace=True)\n",
    "df_predecir.to_parquet('0_b_cliente_producto_autogluon_predecir.parquet', index=False)\n",
    "print(f\"DataFrame para predicción guardado en 'predecir.parquet' con {len(df_predecir)} filas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
