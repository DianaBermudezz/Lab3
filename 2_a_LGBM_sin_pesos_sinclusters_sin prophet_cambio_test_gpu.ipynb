{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "#from sklearn.metrics import cohen_kappa_score, accuracy_score,balanced_accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold # Use KFold for regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#from plotly import express as px\n",
    "\n",
    "#from utils import plot_confusion_matrix\n",
    "\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "\n",
    "from joblib import load, dump\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "import gc\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armado Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./1_c_producto_train.parquet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_DIR = './'\n",
    "\n",
    "#PATH_TO_TRAIN = os.path.join(BASE_DIR, \"Dataset/train.parquet\")\n",
    "#PATH_TO_TRAIN = os.path.join(BASE_DIR, \"Dataset/train_producto.parquet\")\n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"1_c_producto_train.parquet\")\n",
    "PATH_TO_MODELS = os.path.join(BASE_DIR, \"models\")\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"optuna\")\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"optuna\")\n",
    "PATH_TO_MODELS_OLD = os.path.join(BASE_DIR, \"optuna/old/\")\n",
    "\n",
    "\n",
    "nombrebase=\"sqlite:///db.sqlite10115\"\n",
    "nombreestudio=\"producto_sindtw_sin_prophet_estrategia_test_adecuada\"\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 50\n",
    "#TEST_SIZE = 0.2\n",
    "PATH_TO_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(PATH_TO_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset['ultima_tn']\n",
    "del dataset['cluster_500']\n",
    "del dataset['cluster_50']\n",
    "del dataset['cluster_100']\n",
    "del dataset['cluster_10']\n",
    "del dataset['cluster_3']\n",
    "del dataset['cluster_2']\n",
    "del dataset['prod_season_yearly']\n",
    "del dataset['cat_season_yearly']\n",
    "del dataset['total_season_yearly']\n",
    "del dataset['prod_trend']\n",
    "del dataset['cat_trend']\n",
    "del dataset['total_trend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29652, 299)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat1', 'cat2', 'cat3', 'brand', 'categoria', 'estado_producto']\n"
     ]
    }
   ],
   "source": [
    "char_feats = dataset.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(char_feats)\n",
    "#numeric_feats = [f for f in dataset.columns if dataset[f].dtype!='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['periodo', 'product_id', 'producto_total_tn', 'avg_tn', 'std_tn', 'clientes_distintos', 'cust_request_qty', 'cust_request_tn', 'inicio_vida_p', 'fin_vida_p', 'sku_size', 'stock_final', 'dias_mes_4', 'dias_mes_3', 'anio', 'mes', 'trimestre', 'outlier', 'outlier-2', 'tn_lag_1', 'tn_lag_2', 'tn_lag_3', 'tn_lag_4', 'tn_lag_5', 'tn_lag_6', 'tn_lag_7', 'tn_lag_8', 'tn_lag_9', 'tn_lag_10', 'tn_lag_11', 'tn_lag_12', 'tn_lag_13', 'tn_lag_14', 'tn_lag_15', 'tn_lag_16', 'tn_lag_17', 'tn_lag_18', 'tn_lag_19', 'tn_lag_20', 'tn_lag_21', 'tn_lag_22', 'tn_lag_23', 'tn_lag_24', 'tn_lag_25', 'tn_lag_26', 'tn_lag_27', 'tn_lag_28', 'tn_lag_29', 'tn_lag_30', 'tn_lag_31', 'tn_lag_32', 'tn_lag_33', 'tn_lag_34', 'tn_lag_35', 'tn_lag_36', 'tn_media_movil_3', 'tn_media_movil_6', 'tn_media_movil_9', 'tn_media_movil_12', 'tn_std_movil_3', 'tn_std_movil_6', 'tn_std_movil_9', 'tn_std_movil_12', 'tn_min_movil_3', 'tn_min_movil_6', 'tn_min_movil_9', 'tn_min_movil_12', 'delta_media_movil_12', 'delta_media_movil_3', 'delta_media_movil_6', 'delta_media_movil_9', 'delta_std_movil_12', 'delta_std_movil_3', 'delta_std_movil_6', 'delta_std_movil_9', 'delta_min_movil_12', 'delta_min_movil_3', 'delta_min_movil_6', 'delta_min_movil_9', 'delta_tn_1', 'delta_tn_2', 'delta_tn_3', 'delta_tn_4', 'delta_tn_5', 'delta_tn_6', 'delta_tn_7', 'delta_tn_8', 'delta_tn_9', 'delta_tn_10', 'delta_tn_11', 'delta_tn_12', 'delta_tn_13', 'delta_tn_14', 'delta_tn_15', 'delta_tn_16', 'delta_tn_17', 'delta_tn_18', 'delta_tn_19', 'delta_tn_20', 'delta_tn_21', 'delta_tn_22', 'delta_tn_23', 'delta_tn_24', 'delta_tn_25', 'delta_tn_26', 'delta_tn_27', 'delta_tn_28', 'delta_tn_29', 'delta_tn_30', 'delta_tn_31', 'delta_tn_32', 'delta_tn_33', 'delta_tn_34', 'delta_tn_35', 'delta_tn_36', 'total_total_tn', 'total_avg_tn', 'total_std_tn', 'total_min_tn', 'total_max_tn', 'total_productos_distintos', 'total_clientes_distintos', 'total_total_tn_lag_1', 'total_total_tn_diff_1', 'total_total_tn_lag_2', 'total_total_tn_diff_2', 'total_total_tn_lag_3', 'total_total_tn_diff_3', 'total_total_tn_lag_4', 'total_total_tn_diff_4', 'total_total_tn_lag_5', 'total_total_tn_diff_5', 'total_total_tn_lag_6', 'total_total_tn_diff_6', 'total_total_tn_lag_7', 'total_total_tn_diff_7', 'total_total_tn_lag_8', 'total_total_tn_diff_8', 'total_total_tn_lag_9', 'total_total_tn_diff_9', 'total_total_tn_lag_10', 'total_total_tn_diff_10', 'total_total_tn_lag_11', 'total_total_tn_diff_11', 'total_total_tn_lag_12', 'total_total_tn_diff_12', 'total_total_tn_lag_13', 'total_total_tn_diff_13', 'total_total_tn_ma_3', 'total_total_tn_ma_6', 'total_total_tn_ma_12', 'total_total_tn_min_3', 'total_total_tn_min_6', 'total_total_tn_min_12', 'total_total_tn_std_3', 'total_total_tn_std_6', 'total_total_tn_std_12', 'total_total_delta_media_movil_3', 'total_total_delta_media_movil_6', 'total_total_delta_media_movil_12', 'total_total_delta_min_movil_3', 'total_total_delta_min_movil_6', 'total_total_delta_min_movil_12', 'total_total_delta_std_movil_3', 'total_total_delta_std_movil_', 'total_total_delta_std_movil_12', 'clase_producto', 'producto_avg_tn', 'producto_std_tn', 'producto_min_tn', 'producto_max_tn', 'producto_clientes_distintos', 'producto_tn_media_movil_3(con_mes_en_curso)', 'producto_tn_media_movil_3anteriores', 'producto_crecimiento_ventas_suavizado', 'producto_clientes_distintos_lag_1', 'producto_clientes_distintos_growth_1', 'cat_total_tn', 'cat_avg_tn', 'cat_std_tn', 'cat_min_tn', 'cat_max_tn', 'cat_productos_distintos', 'cat_total_tn_lag_1', 'cat_delta_tn_lag_1', 'cat_total_tn_lag_2', 'cat_delta_tn_lag_2', 'cat_total_tn_lag_3', 'cat_delta_tn_lag_3', 'cat_total_tn_lag_4', 'cat_delta_tn_lag_4', 'cat_total_tn_lag_5', 'cat_delta_tn_lag_5', 'cat_total_tn_lag_6', 'cat_delta_tn_lag_6', 'cat_total_tn_lag_7', 'cat_delta_tn_lag_7', 'cat_total_tn_lag_8', 'cat_delta_tn_lag_8', 'cat_total_tn_lag_9', 'cat_delta_tn_lag_9', 'cat_total_tn_lag_10', 'cat_delta_tn_lag_10', 'cat_total_tn_lag_11', 'cat_delta_tn_lag_11', 'cat_total_tn_lag_12', 'cat_delta_tn_lag_12', 'cat_total_tn_lag_13', 'cat_delta_tn_lag_13', 'cat_total_tn_ma_3', 'cat_total_tn_ma_6', 'cat_total_tn_ma_12', 'cat_total_tn_min_3', 'cat_total_tn_min_6', 'cat_total_tn_min_12', 'cat_total_tn_std_3', 'cat_total_tn_std_6', 'cat_total_tn_std_12', 'cat_total_delta_media_movil_3', 'cat_total_delta_media_movil_6', 'cat_total_delta_media_movil_12', 'cat_total_delta_min_movil_3', 'cat_total_delta_min_movil_6', 'cat_total_delta_min_movil_12', 'cat_total_delta_std_movil_3', 'cat_total_delta_std_movil_6', 'cat_total_delta_std_movil_12', 'share_producto_en_categoria', 'share_producto_en_categoria_lag_1', 'share_producto_en_categoria_lag_2', 'share_producto_en_categoria_lag_3', 'share_producto_en_categoria_lag_4', 'share_producto_en_categoria_lag_5', 'share_producto_en_categoria_lag_6', 'share_producto_en_categoria_lag_7', 'share_producto_en_categoria_lag_8', 'share_producto_en_categoria_lag_9', 'share_producto_en_categoria_lag_10', 'share_producto_en_categoria_lag_11', 'share_producto_en_categoria_lag_12', 'share_producto_en_categoria_lag_13', 'tasa_crecimiento_share_producto_en_categoria', 'tasa_crecimiento_share_producto_en_categoria_lag_1', 'tasa_crecimiento_share_producto_en_categoria_lag_2', 'tasa_crecimiento_share_producto_en_categoria_lag_3', 'tasa_crecimiento_share_producto_en_categoria_lag_4', 'tasa_crecimiento_share_producto_en_categoria_lag_5', 'tasa_crecimiento_share_producto_en_categoria_lag_6', 'tasa_crecimiento_share_producto_en_categoria_lag_7', 'tasa_crecimiento_share_producto_en_categoria_lag_8', 'tasa_crecimiento_share_producto_en_categoria_lag_9', 'tasa_crecimiento_share_producto_en_categoria_lag_10', 'tasa_crecimiento_share_producto_en_categoria_lag_11', 'tasa_crecimiento_share_producto_en_categoria_lag_12', 'tasa_crecimiento_share_producto_en_categoria_lag_13', 'share_producto_en_categoria_movil_3(con_mes_en_curso)', 'share_producto_en_categoria_3anteriores', 'share_producto_en_categoria_suavizado', 'meses_vida_producto', 'otros_total_tn_', 'otros_total_tn_lag1', 'otros_total_tn_lag2', 'otros_total_tn_lag3', 'otros_total_tn_lag4', 'otros_total_tn_lag5', 'otros_total_tn_lag6', 'otros_total_tn_lag7', 'otros_total_tn_lag8', 'otros_total_tn_lag9', 'otros_total_tn_lag10', 'otros_total_tn_lag11', 'otros_total_tn_lag12', 'otros_total_tn_lag13', 'otros_avg', 'otros_avg_lag1', 'otros_avg_lag2', 'otros_avg_lag3', 'otros_avg_lag4', 'otros_avg_lag5', 'otros_avg_lag6', 'otros_avg_lag7', 'otros_avg_lag8', 'otros_avg_lag9', 'otros_avg_lag10', 'otros_avg_lag11', 'otros_avg_lag12', 'otros_avg_lag13', 'tn_pred_auto', 'n_meses_hist', 'tn_pred_auto_delta_a_tn', 'ratio_tn_pred_auto_delta_a_tn', 'ratio_tn_pred_a_tn']\n"
     ]
    }
   ],
   "source": [
    "numeric_feats = dataset.select_dtypes(include=['number']).columns.tolist()\n",
    "print(numeric_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29652, 299)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        #   - Datetime\n",
    "        # 1) Datetime → int64 (ns) → float32\n",
    "        if pd.api.types.is_datetime64_any_dtype(col_type):\n",
    "            # view() extrae los nanosegundos desde epoch\n",
    "            df[col] = df[col].view('int64').astype('float32')\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Solo nos ocupamos de numéricos\n",
    "        if not pd.api.types.is_numeric_dtype(col_type):\n",
    "            if not pd.api.types.is_categorical_dtype(col_type):\n",
    "                df[col] = df[col].astype('category')\n",
    "            continue\n",
    "\n",
    "        c_min, c_max = df[col].min(), df[col].max()\n",
    "        has_na = df[col].isnull().any()\n",
    "\n",
    "        # --- ENTEROS ---\n",
    "        if pd.api.types.is_integer_dtype(col_type):\n",
    "            # 1) Sin nulos → numpy ints\n",
    "            if not has_na:\n",
    "                if c_min >= np.iinfo(np.int8).min  and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "\n",
    "\n",
    "        # --- FLOTANTES ---\n",
    "        else:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'Uso de memoria inicial del DataFrame: {start_mem:.2f} MB')\n",
    "        print(f'Uso de memoria final del DataFrame:   {end_mem:.2f} MB')\n",
    "        print(f'Memoria reducida en un {(100*(start_mem-end_mem)/start_mem):.2f}%')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria inicial del DataFrame: 17.98 MB\n",
      "Uso de memoria final del DataFrame:   32.35 MB\n",
      "Memoria reducida en un -79.88%\n"
     ]
    }
   ],
   "source": [
    "dataset = reduce_mem_usage(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29652, 299)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sort_values('periodo')\n",
    "\n",
    "periodos = dataset['periodo'].unique()\n",
    "\n",
    "periodos_train = periodos[:-2]\n",
    "periodos_test  = periodos[-2:]\n",
    "\n",
    "train = dataset[dataset['periodo'].isin(periodos_train)]\n",
    "test  = dataset[dataset['periodo'].isin(periodos_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201701 201702 201703 201704 201705 201706 201707 201708 201709 201710\n",
      " 201711 201712 201801 201802 201803 201804 201805 201806 201807 201808\n",
      " 201809 201810 201811 201812 201901 201902 201903 201904 201905 201906\n",
      " 201907 201908 201909 201910]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['periodo'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201701 201702 201703 201704 201705 201706 201707 201708 201709 201710\n",
      " 201711 201712 201801 201802 201803 201804 201805 201806 201807 201808\n",
      " 201809 201810 201811 201812 201901 201902 201903 201904 201905 201906\n",
      " 201907 201908]\n"
     ]
    }
   ],
   "source": [
    "print(train['periodo'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201909 201910]\n"
     ]
    }
   ],
   "source": [
    "print(test ['periodo'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27764, 299)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar el DataFrame\n",
    "del dataset\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'clase_producto'\n",
    "features = [col for col in train.columns if col != label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(\"train_tmp.parquet\", index=False)\n",
    "del train\n",
    "gc.collect()\n",
    "\n",
    "train = pd.read_parquet(\"train_tmp.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_parquet(\"test_tmp.parquet\", index=False)\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "test = pd.read_parquet(\"test_tmp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27764, 299)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[features]\n",
    "y_train = train[label]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27764, 298)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "#del test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (27764, 298)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial):\n",
    "\n",
    "    # Parámetros para LightGBM\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse', # Cambiado a Mean Squared Error\n",
    "        'verbosity': -1,\n",
    "        'seed': SEED,\n",
    "        # 'num_class': len(y_train.unique()), # Eliminar, esto es para clasificación\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'max_bin': trial.suggest_int('max_bin', 180, 255),\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'num_threads': -1\n",
    "    }\n",
    "\n",
    "    # Voy a generar estimaciones de los 5 modelos del CV sobre los datos test y los acumulo en la matriz scores_ensemble\n",
    "    # Para regresión, scores_ensemble será un array 1D\n",
    "    scores_ensemble = np.zeros(len(y_test),dtype=np.float32)\n",
    "\n",
    "    # Score del 5 fold CV inicializado en 0\n",
    "    score_folds = 0\n",
    "\n",
    "    # Numero de splits del CV\n",
    "    n_splits = 5\n",
    "\n",
    "    # Objeto para hacer el split de CV (usar KFold para regresión, ya que StratifiedKFold es para clasificación)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED) # Added shuffle and random_state for reproducibility\n",
    "    best_iterations = []  # ⬅️ para acumular las best_iteration de cada fold\n",
    "\n",
    "    for i, (if_index, oof_index) in enumerate(kf.split(X_train, y_train)): # Usar kf.split\n",
    "\n",
    "        # Dataset in fold (donde entreno)\n",
    "        lgb_if_dataset = lgb.Dataset(data=X_train.iloc[if_index],\n",
    "                                     label=y_train.iloc[if_index],\n",
    "                                     free_raw_data=True, categorical_feature=char_feats) #cambie free raw data a true\n",
    "\n",
    "        # Dataset Out of fold (donde mido la performance del CV)\n",
    "        lgb_oof_dataset = lgb.Dataset(data=X_train.iloc[oof_index],\n",
    "                                      label=y_train.iloc[oof_index],\n",
    "                                      free_raw_data=True, categorical_feature=char_feats) #cambie free raw data a true\n",
    "\n",
    "        # Entreno el modelo\n",
    "        lgb_model = lgb.train(lgb_params,\n",
    "                              lgb_if_dataset,\n",
    "                              valid_sets=lgb_oof_dataset,\n",
    "                              num_boost_round=10000,\n",
    "                              callbacks=[lgb.early_stopping(200, verbose=False)],\n",
    "                              # feval = mean_squared_error(y_test, preds, squared=False) # Eliminar o definir correctamente para custom metric\n",
    "                             )\n",
    "\n",
    "        # Acumulo las predicciones continuas para el conjunto de test\n",
    "        scores_ensemble = scores_ensemble + lgb_model.predict(X_test)\n",
    "\n",
    "        # Score del fold (registros de dataset train que en este fold quedan out of fold)\n",
    "        # Calcular MSE para el fold OOF\n",
    "        oof_preds = lgb_model.predict(X_train.iloc[oof_index])\n",
    "        score_folds += mean_squared_error(y_train.iloc[oof_index], oof_preds) / n_splits\n",
    "        \n",
    "        # guardo el best_iteration de este fold\n",
    "        best_iterations.append(lgb_model.best_iteration)\n",
    "        \n",
    "        # ⬇️ Liberar memoria de objetos pesados del fold\n",
    "        del lgb_model, lgb_if_dataset, lgb_oof_dataset, oof_preds\n",
    "        gc.collect()\n",
    "       #print(f\"fin split {i}\")\n",
    "\n",
    "    # Promedio las predicciones del ensemble para el conjunto de test\n",
    "    scores_ensemble = scores_ensemble / n_splits\n",
    "\n",
    "    # Guardo prediccion del trial sobre el conjunto de test\n",
    "    # Genero nombre de archivo\n",
    "    #predicted_filename = os.path.join(PATH_TO_TEMP_FILES,f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    # Copia del dataset para guardar la prediccion\n",
    "    #predicted_df = test.copy()\n",
    "    #predicted_df= pd.DataFrame({\n",
    "    #'periodo': test['periodo'],\n",
    "    #'customer_id': test['customer_id'],  # o el ID que necesites\n",
    "    #'product_id' : test['product_id'],\n",
    "    #'pred': scores_ensemble\n",
    "    #})\n",
    "    \n",
    "    # Genero columna pred con predicciones promediadas de los 5 folds\n",
    "    #predicted_df['pred'] = scores_ensemble\n",
    "    # Grabo dataframe en temp_artifacts\n",
    "    #dump(predicted_df, predicted_filename)\n",
    "    # Indico a optuna que asocie el archivo generado al trial\n",
    "    #upload_artifact(trial, predicted_filename, artifact_store)\n",
    "    #del predicted_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Grabo métricas de regresión en lugar de matriz de confusión\n",
    "    # Puedes guardar métricas como MSE, RMSE, MAE, R2, etc. en un archivo de texto o log\n",
    "    # Por ejemplo, calcular el MSE en el conjunto de test\n",
    "    test_mse = mean_squared_error(y_test, scores_ensemble)\n",
    "    trial.set_user_attr(\"test_mse\", test_mse) # Almacenar MSE del test como atributo de usuario\n",
    "\n",
    "    # guardar el promedio de las mejores iteraciones\n",
    "    avg_best_iteration = int(np.mean(best_iterations))\n",
    "    trial.set_user_attr(\"best_iteration\", avg_best_iteration)\n",
    "    \n",
    "    del scores_ensemble\n",
    "    gc.collect()\n",
    "\n",
    "    # Si quieres una visualización, podrías generar un scatter plot de predicciones vs valores reales\n",
    "    # O un histograma de residuos. Aquí un placeholder para un \"regression_metrics.txt\"\n",
    "    #regression_metrics_filename = os.path.join(PATH_TO_TEMP_FILES, f'regression_metrics_{trial.study.study_name}_{trial.number}.txt')\n",
    "    #with open(regression_metrics_filename, 'w') as f:\n",
    "    #    f.write(f\"Test Mean Squared Error: {test_mse}\\n\")\n",
    "    #    # Add other regression metrics if desired\n",
    "    #upload_artifact(trial, regression_metrics_filename, artifact_store)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    # Devuelvo el score promedio de MSE del 5-fold CV a Optuna para que optimice en base a eso\n",
    "    # Optuna minimiza por defecto, por lo que devolver MSE es apropiado.\n",
    "    return score_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-20 22:31:05,290] Using an existing study with name 'producto_sindtw_sin_prophet_estrategia_test_adecuada' instead of creating a new one.\n",
      "[I 2025-07-20 22:32:43,436] Trial 787 finished with value: 708.9021551504188 and parameters: {'lambda_l1': 0.10933081826189021, 'lambda_l2': 0.016306199998440553, 'num_leaves': 12, 'feature_fraction': 0.521800467588188, 'learning_rate': 0.011851100986855765, 'bagging_fraction': 0.8754836194563608, 'bagging_freq': 3, 'min_child_samples': 7, 'max_bin': 186}. Best is trial 545 with value: 696.7639463042119.\n",
      "[W 2025-07-20 22:34:18,812] Trial 788 failed with parameters: {'lambda_l1': 0.3607216612353811, 'lambda_l2': 0.03037563408109612, 'num_leaves': 23, 'feature_fraction': 0.4000166983896206, 'learning_rate': 0.015708904703474243, 'bagging_fraction': 0.8435891251772606, 'bagging_freq': 3, 'min_child_samples': 13, 'max_bin': 223} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/envs/pytorch-gpu/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_677362/1813650926.py\", line 52, in lgb_objective\n",
      "    lgb_model = lgb.train(lgb_params,\n",
      "  File \"/scratch/envs/pytorch-gpu/lib/python3.8/site-packages/lightgbm/engine.py\", line 276, in train\n",
      "    booster.update(fobj=fobj)\n",
      "  File \"/scratch/envs/pytorch-gpu/lib/python3.8/site-packages/lightgbm/basic.py\", line 3891, in update\n",
      "    _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-20 22:34:18,815] Trial 788 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m                             storage\u001b[38;5;241m=\u001b[39mnombrebase,  \u001b[38;5;66;03m# Specify the storage URL here.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                             study_name\u001b[38;5;241m=\u001b[39mnombreestudio,\n\u001b[1;32m      8\u001b[0m                             load_if_exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#Corro la optimizacion\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlgb_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/envs/pytorch-gpu/lib/python3.8/site-packages/optuna/study/study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/envs/pytorch-gpu/lib/python3.8/site-packages/optuna/study/_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/scratch/envs/pytorch-gpu/lib/python3.8/site-packages/optuna/study/_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/scratch/envs/pytorch-gpu/lib/python3.8/site-packages/optuna/study/_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/scratch/envs/pytorch-gpu/lib/python3.8/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[28], line 52\u001b[0m, in \u001b[0;36mlgb_objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     47\u001b[0m lgb_oof_dataset \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(data\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39miloc[oof_index],\n\u001b[1;32m     48\u001b[0m                               label\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39miloc[oof_index],\n\u001b[1;32m     49\u001b[0m                               free_raw_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, categorical_feature\u001b[38;5;241m=\u001b[39mchar_feats) \u001b[38;5;66;03m#cambie free raw data a true\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Entreno el modelo\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlgb_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlgb_if_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlgb_oof_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# feval = mean_squared_error(y_test, preds, squared=False) # Eliminar o definir correctamente para custom metric\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Acumulo las predicciones continuas para el conjunto de test\u001b[39;00m\n\u001b[1;32m     61\u001b[0m scores_ensemble \u001b[38;5;241m=\u001b[39m scores_ensemble \u001b[38;5;241m+\u001b[39m lgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/scratch/envs/pytorch-gpu/lib/python3.8/site-packages/lightgbm/engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    269\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m    270\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    271\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    272\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[1;32m    273\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[1;32m    274\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 276\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/envs/pytorch-gpu/lib/python3.8/site-packages/lightgbm/basic.py:3891\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3890\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3891\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3892\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Inicio el store de artefactos (archivos) de optuna\n",
    "artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "#Genero estudio\n",
    "study = optuna.create_study(direction='minimize',\n",
    "                            storage=nombrebase,  # Specify the storage URL here.\n",
    "                            study_name=nombreestudio,\n",
    "                            load_if_exists = True)\n",
    "#Corro la optimizacion\n",
    "study.optimize(lgb_objective, n_trials=4000, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar modelos con mejores parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar el trial con menor mse_test en los atributos de usuario\n",
    "best_trial = min(\n",
    "    [t for t in study.trials if t.user_attrs.get(\"test_mse\") is not None],\n",
    "    key=lambda t: t.user_attrs[\"test_mse\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=92, state=TrialState.COMPLETE, values=[732.7293916940936], datetime_start=datetime.datetime(2025, 7, 20, 5, 17, 55, 324763), datetime_complete=datetime.datetime(2025, 7, 20, 5, 19, 16, 615253), params={'lambda_l1': 1.556255650932721e-05, 'lambda_l2': 0.027106054470489454, 'num_leaves': 17, 'feature_fraction': 0.6597995610974541, 'learning_rate': 0.027250292209385907, 'bagging_fraction': 0.8773919391074708, 'bagging_freq': 1, 'min_child_samples': 7, 'max_bin': 247}, user_attrs={'best_iteration': 3624, 'test_mse': 651.9364924699062}, system_attrs={}, intermediate_values={}, distributions={'lambda_l1': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'lambda_l2': FloatDistribution(high=10.0, log=True, low=1e-08, step=None), 'num_leaves': IntDistribution(high=256, log=False, low=8, step=1), 'feature_fraction': FloatDistribution(high=1.0, log=False, low=0.4, step=None), 'learning_rate': FloatDistribution(high=0.1, log=False, low=0.01, step=None), 'bagging_fraction': FloatDistribution(high=1.0, log=False, low=0.6, step=None), 'bagging_freq': IntDistribution(high=7, log=False, low=1, step=1), 'min_child_samples': IntDistribution(high=100, log=False, low=5, step=1), 'max_bin': IntDistribution(high=255, log=False, low=128, step=1)}, trial_id=93, value=None)\n"
     ]
    }
   ],
   "source": [
    "print(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_trial.params.copy()\n",
    "best_params.update({\n",
    "    \"objective\": \"regression\",#los parametros que no se optimizan (los fijos) hay que escribirlos\n",
    "    \"metric\": \"mse\",\n",
    "    \"verbosity\": -1,\n",
    "    'seed': SEED    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda_l1': 1.556255650932721e-05, 'lambda_l2': 0.027106054470489454, 'num_leaves': 17, 'feature_fraction': 0.6597995610974541, 'learning_rate': 0.027250292209385907, 'bagging_fraction': 0.8773919391074708, 'bagging_freq': 1, 'min_child_samples': 7, 'max_bin': 247}\n"
     ]
    }
   ],
   "source": [
    "print( best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda_l1': 1.556255650932721e-05, 'lambda_l2': 0.027106054470489454, 'num_leaves': 17, 'feature_fraction': 0.6597995610974541, 'learning_rate': 0.027250292209385907, 'bagging_fraction': 0.8773919391074708, 'bagging_freq': 1, 'min_child_samples': 7, 'max_bin': 247}\n"
     ]
    }
   ],
   "source": [
    "best_iteration = best_trial.user_attrs[\"best_iteration\"]\n",
    "print( best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27764, 298)\n",
      "(1888, 298)\n",
      "(29652, 298)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# Datos completos\n",
    "X_final = pd.concat([X_train, X_test], axis=0)\n",
    "print(X_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27764,)\n",
      "(1888,)\n",
      "(29652, 298)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "y_final = pd.concat([y_train, y_test], axis=0)\n",
    "print(X_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_lgbm_final_todo(base_params, seed):\n",
    "    \"\"\"\n",
    "    Entrena un modelo final en TODO el dataset (train+test),\n",
    "    usando los mejores hiperparámetros y un número fijo de iteraciones.\n",
    "\n",
    "    Args:\n",
    "        base_params (dict): hiperparámetros óptimos (Optuna).\n",
    "        seed (int): semilla para reproducibilidad.\n",
    "\n",
    "    Returns:\n",
    "        modelo final entrenado.\n",
    "    \"\"\"\n",
    "    run_params = base_params.copy()\n",
    "    run_params.update({'seed': seed})\n",
    "\n",
    "    print(\"\\n🚀 Entrenando modelo final en TODO el dataset...\")\n",
    "\n",
    "\n",
    "    all_dataset = lgb.Dataset(\n",
    "        data=X_final,\n",
    "        label=y_final,\n",
    "        free_raw_data=True,\n",
    "        categorical_feature=char_feats\n",
    "    )\n",
    "\n",
    "    # entrenar con el mejor número de iteraciones conocido\n",
    "    # si no lo conocés, podés poner 10000, pero sin early stopping (porque no hay validación)\n",
    "    final_model = lgb.train(\n",
    "        run_params,\n",
    "        train_set=all_dataset,\n",
    "        num_boost_round=best_iteration  # 👈 poné aquí el mejor obtenido antes\n",
    "    )\n",
    "\n",
    "    # Guardar modelo\n",
    "    model_path = os.path.join(PATH_TO_MODELS, f\"lgb_final_model_todo_seed_{seed}.joblib\")\n",
    "    dump(final_model, model_path)\n",
    "    print(f\"✅ Modelo final entrenado y guardado en: {model_path}\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_42.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_5.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_915.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_15.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_666.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_9999.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_37.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_45.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_125.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_90.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1000.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_3.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_753.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_159.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_852.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_10.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_7.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1050.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_654.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_11.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_21.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_33.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_69.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_8008.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_88.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_111.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_222.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_314.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_420.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_512.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_777.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_808.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_999.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1024.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_2048.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_4096.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_17.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_19.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_23.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_29.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_31.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_25.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1234.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_4321.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_867.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1357.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_2468.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_3141.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_2718.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_7000.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_8888.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_4444.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_5555.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_2021.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1984.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_3001.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_6006.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_9090.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_707.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_3333.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_2222.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1759.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_987.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_543.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_876.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_3456.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_7890.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_246.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_135.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_6420.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_9990.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_888.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_7777.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_555.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_444.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_101.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_303.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_909.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_505.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_7070.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_8080.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1100.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1200.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1300.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1400.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1500.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1600.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1700.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1800.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_1900.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_2000.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_2121.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_2323.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_2525.joblib\n",
      "🔁 Modelo movido a backup: lgb_final_model_todo_seed_2727.joblib\n",
      "🔁 Modelo movido a backup: modelo_lgbm_ensemble.joblib\n"
     ]
    }
   ],
   "source": [
    "# Mover modelos previos antes del loop\n",
    "for fname in os.listdir(PATH_TO_MODELS):\n",
    "    if fname.endswith('.joblib'):\n",
    "        src_path = os.path.join(PATH_TO_MODELS, fname)\n",
    "        dst_path = os.path.join(PATH_TO_MODELS_OLD, fname)\n",
    "        shutil.move(src_path, dst_path)\n",
    "        print(f\"🔁 Modelo movido a backup: {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_42.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_5.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_915.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_15.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_666.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_9999.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_37.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_45.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_125.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_90.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1000.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_3.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_753.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_159.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_852.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_10.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_7.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1050.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_654.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_11.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_21.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_33.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_69.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_8008.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_88.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_111.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_222.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_314.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_420.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_512.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_777.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_808.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_999.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1024.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_2048.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_4096.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_17.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_19.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_23.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_29.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_31.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_25.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1234.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_4321.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_867.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1357.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_2468.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_3141.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_2718.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_7000.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_8888.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_4444.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_5555.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_2021.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1984.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_3001.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_6006.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_9090.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_707.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_3333.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_2222.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1759.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_987.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_543.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_876.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_3456.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_7890.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_246.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_135.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_6420.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_9990.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_888.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_7777.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_555.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_444.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_222.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_101.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_303.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_909.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_505.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_7070.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_8080.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1100.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1200.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1300.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1400.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1500.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1600.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1700.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1800.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_1900.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_2000.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_2121.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_2323.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_2525.joblib\n",
      "\n",
      "🚀 Entrenando modelo final en TODO el dataset...\n",
      "✅ Modelo final entrenado y guardado en: ./models/lgb_final_model_todo_seed_2727.joblib\n"
     ]
    }
   ],
   "source": [
    "#results = []\n",
    "#SEEDS = [42,5,915,15,666,9999,37,45,125,90,1000,3,753,159,852,10,7,1,1050,654,11,21,33,69,8008,88,111,222,314,420,512,777,808,999,1024,2048,4096,17,19,23,29,31,25]\n",
    "SEEDS = [\n",
    "    42, 5, 915, 15, 666, 9999, 37, 45, 125, 90,\n",
    "    1000, 3, 753, 159, 852, 10, 7, 1, 1050, 654,\n",
    "    11, 21, 33, 69, 8008, 88, 111, 222, 314, 420,\n",
    "    512, 777, 808, 999, 1024, 2048, 4096, 17, 19,\n",
    "    23, 29, 31, 25,\n",
    "    1234, 4321, 867, 1357, 2468, 3141, 2718, 7000, 8888, 4444,\n",
    "    5555, 2021, 1984, 3001, 6006, 9090, 707, 3333, 2222, 1759,\n",
    "    987, 543, 876, 3456, 7890, 246, 135, 6420, 9990, 888,\n",
    "    7777, 555, 444, 222, 101, 303, 909, 505, 7070, 8080,\n",
    "    1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000,\n",
    "    2121, 2323, 2525, 2727\n",
    "]\n",
    "for current_seed in SEEDS:\n",
    "    # <--- CORRECCIÓN 1 y 2: La función ahora se llama con los parámetros base y la semilla actual\n",
    "    entrenar_lgbm_final_todo(\n",
    "        base_params=best_params, \n",
    "        seed=current_seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch-GPU)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
